{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU Evaluation of LLaMA 3.2 1B Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# no nvlink\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "# use a specific GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.1+cu121\n",
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/llm/llama/Llama-3.2-1B-Instruct/\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MMLU dataset\n",
    "dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14042\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1: 2 shot on simple questions\n",
    "\n",
    "Questions are defined by me, questions that are way too outmatched by the complexity of the task of MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:  \n",
      "Please answer the following user's question in the format \"Answer: ANSWER_TO_THE_QUESTION.\".\n",
      "\n",
      "For example, user input would be:\n",
      "\n",
      "Question: What is 2 + 2?\n",
      "Choices: ['1', '2', '3', '4']\n",
      "\n",
      "Your answer would be:\n",
      "Answer: 4\n",
      "\n",
      "Another example would be:\n",
      "\n",
      "Question: Which one of these parts are used for seeing?\n",
      "Choices: ['eyes', 'nose', 'ears', 'mouth']\n",
      "\n",
      "Your answer would be:\n",
      "Answer: eyes\n",
      "\n",
      "User question:  Question: Which is not a similarity between Saturn and Jupiter's atmospheres?\n",
      "Choices: ['a composition dominated by hydrogen and helium', 'the presence of belts zones and storms', 'an equatorial wind speed of more than 900 miles per hour', 'significant \"shear\" between bands of circulation at different latitudes']\n",
      "Choices:  ['a composition dominated by hydrogen and helium', 'the presence of belts zones and storms', 'an equatorial wind speed of more than 900 miles per hour', 'significant \"shear\" between bands of circulation at different latitudes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 3.21B response:  Answer: ANSWER_TO_THE_question.\n",
      "\n",
      "Explanation: \n",
      "- Saturn's atmosphere is primarily composed of hydrogen and helium.\n",
      "- Jupiter's atmosphere features belts zones and storms.\n",
      "- An equatorial wind speed of more than 900 miles per hour is a characteristic of Jupiter's atmosphere.\n",
      "- Significant \"shear\" between bands of circulation at different latitudes is a characteristic of Saturn's atmosphere.\n",
      "\n",
      "Saturn's atmosphere is different from Jupiter's, as it is primarily composed of hydrogen and helium, whereas Jupiter's atmosphere is dominated by hydrogen and helium.\n",
      "Correct answer:  an equatorial wind speed of more than 900 miles per hour\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "example = dataset[300]\n",
    "question = example['question']\n",
    "choices = example['choices']\n",
    "correct_answer = example['answer']\n",
    "\n",
    "\n",
    "initial_prompt = '''\n",
    "Please answer the following user's question in the format \"Answer: ANSWER_TO_THE_QUESTION.\".\n",
    "\n",
    "For example, user input would be:\n",
    "\n",
    "Question: What is 2 + 2?\n",
    "Choices: ['1', '2', '3', '4']\n",
    "\n",
    "Your answer would be:\n",
    "Answer: 4\n",
    "\n",
    "Another example would be:\n",
    "\n",
    "Question: Which one of these parts are used for seeing?\n",
    "Choices: ['eyes', 'nose', 'ears', 'mouth']\n",
    "\n",
    "Your answer would be:\n",
    "Answer: eyes\n",
    "'''\n",
    "\n",
    "\n",
    "user_question = 'Question: ' + question + \"\\nChoices: \" + str(choices)\n",
    "\n",
    "# question_prompt = input_text\n",
    "print('Initial prompt: ', initial_prompt)\n",
    "print('User question: ', user_question)\n",
    "print('Choices: ', choices)\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': initial_prompt},\n",
    "    {'role': 'user', 'content': user_question},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "output = outputs[0]['generated_text'][-1]['content']\n",
    "print('LLaMA 3.21B response: ', output)\n",
    "print('Correct answer: ', choices[correct_answer])\n",
    "print(choices[correct_answer].lower() in output.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c77169c4794a87aaa4bb6a347d7ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to llama_mmlu_responses.csv\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Assuming the dataset and pipeline are already set up as per your current workflow\n",
    "output_file = 'llama_mmlu_responses.csv'\n",
    "\n",
    "# Sample dataset size, you can adjust it as needed for the loop\n",
    "dataset_size = len(dataset)  # Assuming dataset is loaded with all examples\n",
    "\n",
    "# Initialize a list to collect all rows\n",
    "rows = []\n",
    "\n",
    "# Loop through the dataset and gather question, choices, and answers\n",
    "for idx in tqdm(range(dataset_size)):\n",
    "    example = dataset[idx]\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']\n",
    "    \n",
    "    initial_prompt = '''\n",
    "    Please answer the following user's question in the format \"Answer: ANSWER_TO_THE_QUESTION.\".\n",
    "\n",
    "    For example, user input would be:\n",
    "\n",
    "    Question: What is 2 + 2?\n",
    "    Choices: ['1', '2', '3', '4']\n",
    "\n",
    "    Your answer would be:\n",
    "    Answer: 4\n",
    "\n",
    "    Another example would be:\n",
    "\n",
    "    Question: Which one of these parts are used for seeing?\n",
    "    Choices: ['eyes', 'nose', 'ears', 'mouth']\n",
    "\n",
    "    Your answer would be:\n",
    "    Answer: eyes\n",
    "    '''\n",
    "\n",
    "    user_question = 'Question: ' + question + \"\\nChoices: \" + str(choices)\n",
    "\n",
    "    # Generating messages for the LLaMA model\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': initial_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "\n",
    "    # Get LLaMA's response\n",
    "    outputs = pipe(messages, max_new_tokens=10)\n",
    "    output = outputs[0]['generated_text'][-1]['content']\n",
    "\n",
    "    # Extract LLaMA's answer (post-processed)\n",
    "    answer_start = output.lower().find(\"answer:\") + len(\"answer:\")\n",
    "    llama_answer = output[answer_start:].strip().split()[0]  # Taking the first word as the final answer\n",
    "\n",
    "    # Checking if LLaMA's answer matches the correct answer\n",
    "    is_correct = choices[correct_answer].lower() in output.lower()\n",
    "\n",
    "    # Collect the row\n",
    "    rows.append([question, choices, llama_answer, choices[correct_answer], is_correct])\n",
    "\n",
    "# Write all rows to the CSV file in one go\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 21.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run1.csv')\n",
    "print(f'Average: {float(df[\"Is Correct\"].mean()) * 100:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2: 5-shot on samples taken from MMLU, change format to JSON\n",
    "\n",
    "Randomly sample 5 items from the MMLU dataset, without caring for alignment of the subjects of the samples to the subject of the question. Format response in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MMLU dataset\n",
    "train_dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morning sickness is typically a problem: ['during the first trimester', 'during the second trimester', 'during the third trimester', 'all through the pregnancy'] during the first trimester\n",
      "The population of the city where Michelle was born is 145,826. What is the value of the 5 in the number 145,826? ['5 thousands', '5 hundreds', '5 tens', '5 ones'] 5 thousands\n",
      "Which element in tobacco smoke is responsible for cancers? ['Nicotine', 'Tar', 'Carbon monoxide', 'Smoke particles'] Tar\n",
      "What is the embryological origin of the hyoid bone? ['The first pharyngeal arch', 'The first and second pharyngeal arches', 'The second pharyngeal arch', 'The second and third pharyngeal arches'] The second and third pharyngeal arches\n",
      "3 Cl−(aq) + 4 CrO_4^2−(aq) + 23 H+(aq) → 3 HClO2(aq) + 4 Cr3+(aq) + 10 H2O(l). In the reaction shown above, Cl−(aq) behaves as ['an acid', 'a base', 'a catalyst', 'a reducing agent'] a reducing agent\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n_shots = 5\n",
    "train_indices = random.sample(range(len(train_dataset)), n_shots)  # Get random indices\n",
    "few_shot_examples = train_dataset.select(train_indices)  # Select the few-shot examples\n",
    "\n",
    "for example in few_shot_examples:\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = choices[example['answer']]\n",
    "    print(question, choices, correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a bot that only answers the following question in JSON format.\n",
      "\n",
      "For example, if the question is:\n",
      "\n",
      "Question: Which one of the following items is an example of nonmaterial culture?\n",
      "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
      "\n",
      "Your answer would be:\n",
      "{\"Answer\": \"Dove symbol\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
      "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"5.78\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
      "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"Not wrong, Wrong\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
      "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"increased, decreased\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
      " IF (X < O)\n",
      " {\n",
      " DISPLAY (\"Foxtrot\")\n",
      " }\n",
      " ELSE\n",
      " {\n",
      " IF (X > y)\n",
      " {\n",
      "   DISPLAY (\"Hotel\")\n",
      " }\n",
      " ELSE \n",
      " {\n",
      "   IF (y > O)\n",
      "   {\n",
      "   DISPLAY (\"November\")\n",
      "   }\n",
      "   ELSE\n",
      "   {\n",
      "   DISPLAY (\"Yankee\")\n",
      "   }\n",
      " }\n",
      " }\n",
      "What is displayed as a result of running the program?\n",
      "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"November\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load your dataset and set up the model pipeline\n",
    "output_file = 'llama_mmlu_responses_run2.csv'\n",
    "\n",
    "# Ensure GPU is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize your LLaMA model pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for faster GPU performance\n",
    "    device=0,  # Use the first GPU\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Sample n_shots indices from the training set\n",
    "train_indices = random.sample(range(len(train_dataset)), n_shots)  # Get random indices\n",
    "few_shot_examples = train_dataset.select(train_indices)  # Select the few-shot examples\n",
    "\n",
    "# Create the few-shot prompt\n",
    "\n",
    "few_shot_prompt = '''\n",
    "You are a bot that only answers the following question in JSON format.\n",
    "\n",
    "For example, if the question is:\n",
    "\n",
    "Question: Which one of the following items is an example of nonmaterial culture?\n",
    "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
    "\n",
    "Your answer would be:\n",
    "{\"Answer\": \"Dove symbol\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
    "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"5.78\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
    "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"Not wrong, Wrong\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
    "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"increased, decreased\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
    " IF (X < O)\n",
    " {\n",
    " DISPLAY (\"Foxtrot\")\n",
    " }\n",
    " ELSE\n",
    " {\n",
    " IF (X > y)\n",
    " {\n",
    "   DISPLAY (\"Hotel\")\n",
    " }\n",
    " ELSE \n",
    " {\n",
    "   IF (y > O)\n",
    "   {\n",
    "   DISPLAY (\"November\")\n",
    "   }\n",
    "   ELSE\n",
    "   {\n",
    "   DISPLAY (\"Yankee\")\n",
    "   }\n",
    " }\n",
    " }\n",
    "What is displayed as a result of running the program?\n",
    "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"November\"}\n",
    "\n",
    "'''\n",
    "# for example in few_shot_examples:\n",
    "#     question = example['question']\n",
    "#     choices = example['choices']\n",
    "#     correct_answer = choices[example['answer']]\n",
    "    \n",
    "#     few_shot_prompt += f\"Question: {question}\\nChoices: {choices}\\nAnswer: {correct_answer}\\n\\n\"\n",
    "\n",
    "print(few_shot_prompt)\n",
    "# Initialize a list to collect all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1436a23667ec468786afa02422047892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing evaluation set:   0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 0 Correct Answers\n",
      "11 / 100 Correct Answers\n",
      "38 / 200 Correct Answers\n",
      "53 / 300 Correct Answers\n",
      "69 / 400 Correct Answers\n",
      "89 / 500 Correct Answers\n",
      "100 / 600 Correct Answers\n",
      "113 / 700 Correct Answers\n",
      "127 / 800 Correct Answers\n",
      "137 / 900 Correct Answers\n",
      "153 / 1000 Correct Answers\n",
      "172 / 1100 Correct Answers\n",
      "184 / 1200 Correct Answers\n",
      "195 / 1300 Correct Answers\n",
      "212 / 1400 Correct Answers\n",
      "231 / 1500 Correct Answers\n",
      "257 / 1600 Correct Answers\n",
      "284 / 1700 Correct Answers\n",
      "306 / 1800 Correct Answers\n",
      "311 / 1900 Correct Answers\n",
      "331 / 2000 Correct Answers\n",
      "356 / 2100 Correct Answers\n",
      "378 / 2200 Correct Answers\n",
      "396 / 2300 Correct Answers\n",
      "419 / 2400 Correct Answers\n",
      "434 / 2500 Correct Answers\n",
      "443 / 2600 Correct Answers\n",
      "473 / 2700 Correct Answers\n",
      "485 / 2800 Correct Answers\n",
      "496 / 2900 Correct Answers\n",
      "511 / 3000 Correct Answers\n",
      "517 / 3100 Correct Answers\n",
      "530 / 3200 Correct Answers\n",
      "543 / 3300 Correct Answers\n",
      "549 / 3400 Correct Answers\n",
      "572 / 3500 Correct Answers\n",
      "597 / 3600 Correct Answers\n",
      "617 / 3700 Correct Answers\n",
      "630 / 3800 Correct Answers\n",
      "639 / 3900 Correct Answers\n",
      "646 / 4000 Correct Answers\n",
      "650 / 4100 Correct Answers\n",
      "661 / 4200 Correct Answers\n",
      "673 / 4300 Correct Answers\n",
      "688 / 4400 Correct Answers\n",
      "701 / 4500 Correct Answers\n",
      "705 / 4600 Correct Answers\n",
      "713 / 4700 Correct Answers\n",
      "720 / 4800 Correct Answers\n",
      "729 / 4900 Correct Answers\n",
      "759 / 5000 Correct Answers\n",
      "795 / 5100 Correct Answers\n",
      "830 / 5200 Correct Answers\n",
      "867 / 5300 Correct Answers\n",
      "889 / 5400 Correct Answers\n",
      "895 / 5500 Correct Answers\n",
      "903 / 5600 Correct Answers\n",
      "913 / 5700 Correct Answers\n",
      "927 / 5800 Correct Answers\n",
      "941 / 5900 Correct Answers\n",
      "952 / 6000 Correct Answers\n",
      "961 / 6100 Correct Answers\n",
      "989 / 6200 Correct Answers\n",
      "1016 / 6300 Correct Answers\n",
      "1044 / 6400 Correct Answers\n",
      "1052 / 6500 Correct Answers\n",
      "1059 / 6600 Correct Answers\n",
      "1070 / 6700 Correct Answers\n",
      "1087 / 6800 Correct Answers\n",
      "1108 / 6900 Correct Answers\n",
      "1130 / 7000 Correct Answers\n",
      "1166 / 7100 Correct Answers\n",
      "1204 / 7200 Correct Answers\n",
      "1239 / 7300 Correct Answers\n",
      "1260 / 7400 Correct Answers\n",
      "1295 / 7500 Correct Answers\n",
      "1327 / 7600 Correct Answers\n",
      "1367 / 7700 Correct Answers\n",
      "1402 / 7800 Correct Answers\n",
      "1446 / 7900 Correct Answers\n",
      "1485 / 8000 Correct Answers\n",
      "1515 / 8100 Correct Answers\n",
      "1545 / 8200 Correct Answers\n",
      "1555 / 8300 Correct Answers\n",
      "1570 / 8400 Correct Answers\n",
      "1585 / 8500 Correct Answers\n",
      "1595 / 8600 Correct Answers\n",
      "1615 / 8700 Correct Answers\n",
      "1630 / 8800 Correct Answers\n",
      "1646 / 8900 Correct Answers\n",
      "1670 / 9000 Correct Answers\n",
      "1682 / 9100 Correct Answers\n",
      "1694 / 9200 Correct Answers\n",
      "1706 / 9300 Correct Answers\n",
      "1724 / 9400 Correct Answers\n",
      "1738 / 9500 Correct Answers\n",
      "1746 / 9600 Correct Answers\n",
      "1760 / 9700 Correct Answers\n",
      "1779 / 9800 Correct Answers\n",
      "1793 / 9900 Correct Answers\n",
      "1810 / 10000 Correct Answers\n",
      "1821 / 10100 Correct Answers\n",
      "1832 / 10200 Correct Answers\n",
      "1852 / 10300 Correct Answers\n",
      "1875 / 10400 Correct Answers\n",
      "1890 / 10500 Correct Answers\n",
      "1905 / 10600 Correct Answers\n",
      "1909 / 10700 Correct Answers\n",
      "1911 / 10800 Correct Answers\n",
      "1913 / 10900 Correct Answers\n",
      "1914 / 11000 Correct Answers\n",
      "1916 / 11100 Correct Answers\n",
      "1916 / 11200 Correct Answers\n",
      "1918 / 11300 Correct Answers\n",
      "1919 / 11400 Correct Answers\n",
      "1921 / 11500 Correct Answers\n",
      "1923 / 11600 Correct Answers\n",
      "1925 / 11700 Correct Answers\n",
      "1931 / 11800 Correct Answers\n",
      "1934 / 11900 Correct Answers\n",
      "1936 / 12000 Correct Answers\n",
      "1938 / 12100 Correct Answers\n",
      "1941 / 12200 Correct Answers\n",
      "1954 / 12300 Correct Answers\n",
      "1964 / 12400 Correct Answers\n",
      "1974 / 12500 Correct Answers\n",
      "1987 / 12600 Correct Answers\n",
      "2004 / 12700 Correct Answers\n",
      "2012 / 12800 Correct Answers\n",
      "2022 / 12900 Correct Answers\n",
      "2031 / 13000 Correct Answers\n",
      "2053 / 13100 Correct Answers\n",
      "2073 / 13200 Correct Answers\n",
      "2075 / 13300 Correct Answers\n",
      "2078 / 13400 Correct Answers\n",
      "2090 / 13500 Correct Answers\n",
      "2104 / 13600 Correct Answers\n",
      "2121 / 13700 Correct Answers\n",
      "2135 / 13800 Correct Answers\n",
      "2153 / 13900 Correct Answers\n",
      "2187 / 14000 Correct Answers\n",
      "Results saved to llama_mmlu_responses_run2.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Processing the evaluation set with tqdm for a progress bar\n",
    "rows = []\n",
    "is_correct_count = 0\n",
    "\n",
    "for idx in tqdm(range(0, len(dataset)), desc=\"Processing evaluation set\"):\n",
    "    format_error = 'None'\n",
    "    example = dataset[idx]\n",
    "\n",
    "    # Prepare the user question with few-shot prompt\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']\n",
    "    \n",
    "    stringified_choices = re.sub('\"', '\\'', str(choices))\n",
    "    # Append the new question to the few-shot prompt\n",
    "    user_question = f\"Question: {question}\\nChoices: {stringified_choices}\\n\"\n",
    "    \n",
    "    # Messages to be passed to the model\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': few_shot_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    \n",
    "    # Run the model with torch.cuda.amp for mixed precision (parallel processing)\n",
    "    outputs = pipe(messages, max_new_tokens=10)\n",
    "    \n",
    "    output = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    # Extract LLaMA's answer (post-processed)\n",
    "    llama_answer = output.lower()\n",
    "    try:\n",
    "        llama_answer = json.loads(llama_answer)['answer']\n",
    "    except Exception as e:\n",
    "        format_error = 'Answer is not in JSON format.'\n",
    "\n",
    "\n",
    "    # Check if the answer matches the correct answer\n",
    "    if type(llama_answer) == str:\n",
    "        is_correct = choices[correct_answer].lower() == re.sub('\"', '\\'', llama_answer).lower()\n",
    "    else:\n",
    "        is_correct = False\n",
    "        format_error = 'Parsed answer is still dictionary.'\n",
    "\n",
    "    is_correct_count += 1 if is_correct else 0\n",
    "    if idx % 100 == 0:\n",
    "        print(f'{is_correct_count} / {idx} Correct Answers')\n",
    "    # Collect the result row\n",
    "    rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, format_error])\n",
    "\n",
    "# Write the collected results to a CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\", \"Format Error\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 15.71713431135166\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run2.csv')\n",
    "\n",
    "print(f'Average: {float(df[\"Is Correct\"].mean()) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 3: Try fixing the temperature\n",
    "\n",
    "Temperature set at 0.1 for deterministic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a bot that only answers the following question in JSON format.\n",
      "\n",
      "For example, if the question is:\n",
      "\n",
      "Question: Which one of the following items is an example of nonmaterial culture?\n",
      "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
      "\n",
      "Your answer would be:\n",
      "{\"Answer\": \"Dove symbol\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
      "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"5.78\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
      "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"Not wrong, Wrong\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
      "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"increased, decreased\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
      " IF (X < O)\n",
      " {\n",
      " DISPLAY (\"Foxtrot\")\n",
      " }\n",
      " ELSE\n",
      " {\n",
      " IF (X > y)\n",
      " {\n",
      "   DISPLAY (\"Hotel\")\n",
      " }\n",
      " ELSE \n",
      " {\n",
      "   IF (y > O)\n",
      "   {\n",
      "   DISPLAY (\"November\")\n",
      "   }\n",
      "   ELSE\n",
      "   {\n",
      "   DISPLAY (\"Yankee\")\n",
      "   }\n",
      " }\n",
      " }\n",
      "What is displayed as a result of running the program?\n",
      "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"November\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Ensure GPU is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize your LLaMA model pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for faster GPU performance\n",
    "    device=0,  # Use the first GPU\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "low_temp_few_shot_prompt = '''\n",
    "You are a bot that only answers the following question in JSON format.\n",
    "\n",
    "For example, if the question is:\n",
    "\n",
    "Question: Which one of the following items is an example of nonmaterial culture?\n",
    "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
    "\n",
    "Your answer would be:\n",
    "{\"Answer\": \"Dove symbol\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
    "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"5.78\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
    "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"Not wrong, Wrong\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
    "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"increased, decreased\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
    " IF (X < O)\n",
    " {\n",
    " DISPLAY (\"Foxtrot\")\n",
    " }\n",
    " ELSE\n",
    " {\n",
    " IF (X > y)\n",
    " {\n",
    "   DISPLAY (\"Hotel\")\n",
    " }\n",
    " ELSE \n",
    " {\n",
    "   IF (y > O)\n",
    "   {\n",
    "   DISPLAY (\"November\")\n",
    "   }\n",
    "   ELSE\n",
    "   {\n",
    "   DISPLAY (\"Yankee\")\n",
    "   }\n",
    " }\n",
    " }\n",
    "What is displayed as a result of running the program?\n",
    "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"November\"}\n",
    "'''\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14869fb84f7a4391b23cc1fa501c21a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing evaluation set:   0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 0 Correct Answers\n",
      "19 / 100 Correct Answers\n",
      "62 / 200 Correct Answers\n",
      "98 / 300 Correct Answers\n",
      "131 / 400 Correct Answers\n",
      "164 / 500 Correct Answers\n",
      "195 / 600 Correct Answers\n",
      "226 / 700 Correct Answers\n",
      "268 / 800 Correct Answers\n",
      "295 / 900 Correct Answers\n",
      "315 / 1000 Correct Answers\n",
      "330 / 1100 Correct Answers\n",
      "344 / 1200 Correct Answers\n",
      "377 / 1300 Correct Answers\n",
      "405 / 1400 Correct Answers\n",
      "426 / 1500 Correct Answers\n",
      "460 / 1600 Correct Answers\n",
      "490 / 1700 Correct Answers\n",
      "522 / 1800 Correct Answers\n",
      "535 / 1900 Correct Answers\n",
      "569 / 2000 Correct Answers\n",
      "603 / 2100 Correct Answers\n",
      "628 / 2200 Correct Answers\n",
      "659 / 2300 Correct Answers\n",
      "683 / 2400 Correct Answers\n",
      "708 / 2500 Correct Answers\n",
      "730 / 2600 Correct Answers\n",
      "763 / 2700 Correct Answers\n",
      "800 / 2800 Correct Answers\n",
      "832 / 2900 Correct Answers\n",
      "860 / 3000 Correct Answers\n",
      "886 / 3100 Correct Answers\n",
      "918 / 3200 Correct Answers\n",
      "949 / 3300 Correct Answers\n",
      "982 / 3400 Correct Answers\n",
      "1023 / 3500 Correct Answers\n",
      "1062 / 3600 Correct Answers\n",
      "1099 / 3700 Correct Answers\n",
      "1139 / 3800 Correct Answers\n",
      "1170 / 3900 Correct Answers\n",
      "1193 / 4000 Correct Answers\n",
      "1217 / 4100 Correct Answers\n",
      "1249 / 4200 Correct Answers\n",
      "1270 / 4300 Correct Answers\n",
      "1297 / 4400 Correct Answers\n",
      "1324 / 4500 Correct Answers\n",
      "1363 / 4600 Correct Answers\n",
      "1400 / 4700 Correct Answers\n",
      "1420 / 4800 Correct Answers\n",
      "1437 / 4900 Correct Answers\n",
      "1477 / 5000 Correct Answers\n",
      "1523 / 5100 Correct Answers\n",
      "1563 / 5200 Correct Answers\n",
      "1605 / 5300 Correct Answers\n",
      "1643 / 5400 Correct Answers\n",
      "1661 / 5500 Correct Answers\n",
      "1683 / 5600 Correct Answers\n",
      "1713 / 5700 Correct Answers\n",
      "1749 / 5800 Correct Answers\n",
      "1781 / 5900 Correct Answers\n",
      "1819 / 6000 Correct Answers\n",
      "1854 / 6100 Correct Answers\n",
      "1903 / 6200 Correct Answers\n",
      "1947 / 6300 Correct Answers\n",
      "1983 / 6400 Correct Answers\n",
      "2012 / 6500 Correct Answers\n",
      "2041 / 6600 Correct Answers\n",
      "2078 / 6700 Correct Answers\n",
      "2099 / 6800 Correct Answers\n",
      "2128 / 6900 Correct Answers\n",
      "2165 / 7000 Correct Answers\n",
      "2215 / 7100 Correct Answers\n",
      "2271 / 7200 Correct Answers\n",
      "2322 / 7300 Correct Answers\n",
      "2357 / 7400 Correct Answers\n",
      "2404 / 7500 Correct Answers\n",
      "2445 / 7600 Correct Answers\n",
      "2491 / 7700 Correct Answers\n",
      "2541 / 7800 Correct Answers\n",
      "2595 / 7900 Correct Answers\n",
      "2640 / 8000 Correct Answers\n",
      "2696 / 8100 Correct Answers\n",
      "2734 / 8200 Correct Answers\n",
      "2773 / 8300 Correct Answers\n",
      "2808 / 8400 Correct Answers\n",
      "2842 / 8500 Correct Answers\n",
      "2864 / 8600 Correct Answers\n",
      "2889 / 8700 Correct Answers\n",
      "2913 / 8800 Correct Answers\n",
      "2937 / 8900 Correct Answers\n",
      "2963 / 9000 Correct Answers\n",
      "2991 / 9100 Correct Answers\n",
      "3014 / 9200 Correct Answers\n",
      "3034 / 9300 Correct Answers\n",
      "3062 / 9400 Correct Answers\n",
      "3089 / 9500 Correct Answers\n",
      "3120 / 9600 Correct Answers\n",
      "3150 / 9700 Correct Answers\n",
      "3185 / 9800 Correct Answers\n",
      "3218 / 9900 Correct Answers\n",
      "3255 / 10000 Correct Answers\n",
      "3284 / 10100 Correct Answers\n",
      "3317 / 10200 Correct Answers\n",
      "3355 / 10300 Correct Answers\n",
      "3391 / 10400 Correct Answers\n",
      "3415 / 10500 Correct Answers\n",
      "3447 / 10600 Correct Answers\n",
      "3470 / 10700 Correct Answers\n",
      "3488 / 10800 Correct Answers\n",
      "3512 / 10900 Correct Answers\n",
      "3527 / 11000 Correct Answers\n",
      "3545 / 11100 Correct Answers\n",
      "3559 / 11200 Correct Answers\n",
      "3568 / 11300 Correct Answers\n",
      "3576 / 11400 Correct Answers\n",
      "3593 / 11500 Correct Answers\n",
      "3615 / 11600 Correct Answers\n",
      "3629 / 11700 Correct Answers\n",
      "3644 / 11800 Correct Answers\n",
      "3657 / 11900 Correct Answers\n",
      "3675 / 12000 Correct Answers\n",
      "3694 / 12100 Correct Answers\n",
      "3710 / 12200 Correct Answers\n",
      "3735 / 12300 Correct Answers\n",
      "3746 / 12400 Correct Answers\n",
      "3772 / 12500 Correct Answers\n",
      "3805 / 12600 Correct Answers\n",
      "3832 / 12700 Correct Answers\n",
      "3861 / 12800 Correct Answers\n",
      "3894 / 12900 Correct Answers\n",
      "3926 / 13000 Correct Answers\n",
      "3957 / 13100 Correct Answers\n",
      "3993 / 13200 Correct Answers\n",
      "4013 / 13300 Correct Answers\n",
      "4034 / 13400 Correct Answers\n",
      "4074 / 13500 Correct Answers\n",
      "4108 / 13600 Correct Answers\n",
      "4157 / 13700 Correct Answers\n",
      "4189 / 13800 Correct Answers\n",
      "4223 / 13900 Correct Answers\n",
      "4262 / 14000 Correct Answers\n",
      "Results saved to llama_mmlu_responses_5shot_lowtemp.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Processing the evaluation set with tqdm for a progress bar\n",
    "rows = []\n",
    "is_correct_count = 0\n",
    "output_file = 'llama_mmlu_responses_run3.csv'\n",
    "\n",
    "for idx in tqdm(range(0, len(dataset)), desc=\"Processing evaluation set\"):\n",
    "    format_error = 'None'\n",
    "    example = dataset[idx]\n",
    "\n",
    "    # Prepare the user question with few-shot prompt\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']\n",
    "    \n",
    "    stringified_choices = re.sub('\"', '\\'', str(choices))\n",
    "    # Append the new question to the few-shot prompt\n",
    "    user_question = f\"Question: {question}\\nChoices: {stringified_choices}\\n\"\n",
    "    \n",
    "    # Messages to be passed to the model\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': low_temp_few_shot_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    \n",
    "    # Run the model with torch.cuda.amp for mixed precision (parallel processing)\n",
    "    outputs = pipe(messages, max_new_tokens=256, temperature=0.1)\n",
    "    \n",
    "    output = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    # Extract LLaMA's answer (post-processed)\n",
    "    llama_answer = output.lower()\n",
    "    try:\n",
    "        llama_answer = json.loads(llama_answer)['answer']\n",
    "    except Exception as e:\n",
    "        format_error = 'Answer is not in JSON format.'\n",
    "\n",
    "\n",
    "    # Check if the answer matches the correct answer\n",
    "    if type(llama_answer) == str:\n",
    "        is_correct = choices[correct_answer].lower() == re.sub('\"', '\\'', llama_answer).lower()\n",
    "    else:\n",
    "        is_correct = False\n",
    "        format_error = 'Parsed answer is still dictionary.'\n",
    "\n",
    "    is_correct_count += 1 if is_correct else 0\n",
    "    if idx % 100 == 0:\n",
    "        print(f'{is_correct_count} / {idx} Correct Answers')\n",
    "    # Collect the result row\n",
    "    rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, format_error])\n",
    "\n",
    "# Write the collected results to a CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\", \"Format Error\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 30.4871100982766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run3.csv')\n",
    "\n",
    "print(f'Average: {float(df[\"Is Correct\"].mean()) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the discrepancies might come from\n",
    "\n",
    "1. Perhaps increase max tokens generated in prompt?\n",
    "2. Perhaps simplify the prompt even further?\n",
    "3. Perhaps compartmentalize the querying of the model on each subject?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 4: Increase tokens in generated result\n",
    "\n",
    "Set to max_token=256.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a bot that only answers the following question in JSON format.\n",
      "\n",
      "For example, if the question is:\n",
      "\n",
      "Question: Which one of the following items is an example of nonmaterial culture?\n",
      "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
      "\n",
      "Your answer would be:\n",
      "{\"Answer\": \"Dove symbol\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
      "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"5.78\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
      "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"Not wrong, Wrong\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
      "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"increased, decreased\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Another example question is:\n",
      "\n",
      "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
      " IF (X < O)\n",
      " {\n",
      " DISPLAY (\"Foxtrot\")\n",
      " }\n",
      " ELSE\n",
      " {\n",
      " IF (X > y)\n",
      " {\n",
      "   DISPLAY (\"Hotel\")\n",
      " }\n",
      " ELSE \n",
      " {\n",
      "   IF (y > O)\n",
      "   {\n",
      "   DISPLAY (\"November\")\n",
      "   }\n",
      "   ELSE\n",
      "   {\n",
      "   DISPLAY (\"Yankee\")\n",
      "   }\n",
      " }\n",
      " }\n",
      "What is displayed as a result of running the program?\n",
      "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
      "\n",
      "And your answer would be:\n",
      "{\"Answer\": \"November\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load your dataset and set up the model pipeline\n",
    "# output_file = 'llama_mmlu_responses_5shot.csv'\n",
    "\n",
    "# Ensure GPU is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize your LLaMA model pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for faster GPU performance\n",
    "    device=0,  # Use the first GPU\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Sample n_shots indices from the training set\n",
    "# train_indices = random.sample(range(len(train_dataset)), n_shots)  # Get random indices\n",
    "# few_shot_examples = train_dataset.select(train_indices)  # Select the few-shot examples\n",
    "\n",
    "# Create the few-shot prompt\n",
    "\n",
    "few_shot_prompt = '''\n",
    "You are a bot that only answers the following question in JSON format.\n",
    "\n",
    "For example, if the question is:\n",
    "\n",
    "Question: Which one of the following items is an example of nonmaterial culture?\n",
    "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
    "\n",
    "Your answer would be:\n",
    "{\"Answer\": \"Dove symbol\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
    "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"5.78\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
    "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"Not wrong, Wrong\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
    "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"increased, decreased\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Another example question is:\n",
    "\n",
    "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
    " IF (X < O)\n",
    " {\n",
    " DISPLAY (\"Foxtrot\")\n",
    " }\n",
    " ELSE\n",
    " {\n",
    " IF (X > y)\n",
    " {\n",
    "   DISPLAY (\"Hotel\")\n",
    " }\n",
    " ELSE \n",
    " {\n",
    "   IF (y > O)\n",
    "   {\n",
    "   DISPLAY (\"November\")\n",
    "   }\n",
    "   ELSE\n",
    "   {\n",
    "   DISPLAY (\"Yankee\")\n",
    "   }\n",
    " }\n",
    " }\n",
    "What is displayed as a result of running the program?\n",
    "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
    "\n",
    "And your answer would be:\n",
    "{\"Answer\": \"November\"}\n",
    "'''\n",
    "# for example in few_shot_examples:\n",
    "#     question = example['question']\n",
    "#     choices = example['choices']\n",
    "#     correct_answer = choices[example['answer']]\n",
    "    \n",
    "#     few_shot_prompt += f\"Question: {question}\\nChoices: {choices}\\nAnswer: {correct_answer}\\n\\n\"\n",
    "\n",
    "print(few_shot_prompt)\n",
    "# Initialize a list to collect all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7d387f0b5a45c692fc050d1732f3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing evaluation set:   0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 0 Correct Answers\n",
      "19 / 100 Correct Answers\n",
      "69 / 200 Correct Answers\n",
      "Results saved to llama_mmlu_responses_run5.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Processing the evaluation set with tqdm for a progress bar\n",
    "rows = []\n",
    "is_correct_count = 0\n",
    "output_file = 'llama_mmlu_responses_run4.csv'\n",
    "\n",
    "for idx in tqdm(range(0, len(train_dataset)), desc=\"Processing evaluation set\"):\n",
    "    format_error = 'None'\n",
    "    example = dataset[idx]\n",
    "\n",
    "    # Prepare the user question with few-shot prompt\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']\n",
    "    \n",
    "    stringified_choices = re.sub('\"', '\\'', str(choices))\n",
    "    # Append the new question to the few-shot prompt\n",
    "    user_question = f\"Question: {question}\\nChoices: {stringified_choices}\\n\"\n",
    "    \n",
    "    # Messages to be passed to the model\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': few_shot_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    outputs = pipe(messages, max_new_tokens=256)\n",
    "    output = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    # Extract LLaMA's answer (post-processed)\n",
    "    llama_answer = output.lower()\n",
    "    try:\n",
    "        llama_answer = json.loads(llama_answer)['answer']\n",
    "    except Exception as e:\n",
    "        format_error = 'Answer is not in JSON format.'\n",
    "\n",
    "\n",
    "    # Check if the answer matches the correct answer\n",
    "    if type(llama_answer) == str:\n",
    "        is_correct = choices[correct_answer].lower() == re.sub('\"', '\\'', llama_answer).lower()\n",
    "    else:\n",
    "        is_correct = False\n",
    "        format_error = 'Parsed answer is still dictionary.'\n",
    "\n",
    "    is_correct_count += 1 if is_correct else 0\n",
    "    if idx % 100 == 0:\n",
    "        print(f'{is_correct_count} / {idx} Correct Answers')\n",
    "    # Collect the result row\n",
    "    rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, format_error])\n",
    "\n",
    "# Write the collected results to a CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\", \"Format Error\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 35.78947368421053\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run4.csv')\n",
    "\n",
    "print(f'Average: {float(df[\"Is Correct\"].mean()) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 5: Use a more directed 5-shot prompt\n",
    "\n",
    "Removed some extra spaces and newlines. End with a final instruction: \"After here, you will be given questions. Select the best answer from the choices provided. Do not explain your answer. Format your answer as with the examples above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a bot that's trained to answer questions in JSON format. For example:\n",
      "\n",
      "Question: Which one of the following items is an example of nonmaterial culture?\n",
      "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
      "{\"Answer\": \"Dove symbol\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
      "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
      "{\"Answer\": 5.78}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
      "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
      "{\"Answer\": \"Not wrong, Wrong\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
      "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
      "{\"Answer\": \"increased, decreased\"}\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
      " IF (X < O)\n",
      " {\n",
      " DISPLAY (\"Foxtrot\")\n",
      " }\n",
      " ELSE\n",
      " {\n",
      " IF (X > y)\n",
      " {\n",
      "   DISPLAY (\"Hotel\")\n",
      " }\n",
      " ELSE \n",
      " {\n",
      "   IF (y > O)\n",
      "   {\n",
      "   DISPLAY (\"November\")\n",
      "   }\n",
      "   ELSE\n",
      "   {\n",
      "   DISPLAY (\"Yankee\")\n",
      "   }\n",
      " }\n",
      " }\n",
      "What is displayed as a result of running the program?\n",
      "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
      "{\"Answer\": \"November\"}\n",
      "\n",
      "After here, you will be given questions. Select the best answer from the choices provided. Do not explain your answer. Format your answer as with the examples above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load your dataset and set up the model pipeline\n",
    "# output_file = 'llama_mmlu_responses_5shot.csv'\n",
    "\n",
    "# Ensure GPU is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize your LLaMA model pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for faster GPU performance\n",
    "    device=0,  # Use the first GPU\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Sample n_shots indices from the training set\n",
    "# train_indices = random.sample(range(len(train_dataset)), n_shots)  # Get random indices\n",
    "# few_shot_examples = train_dataset.select(train_indices)  # Select the few-shot examples\n",
    "\n",
    "# Create the few-shot prompt\n",
    "\n",
    "few_shot_prompt = '''\n",
    "You are a bot that's trained to answer questions in JSON format. For example:\n",
    "\n",
    "Question: Which one of the following items is an example of nonmaterial culture?\n",
    "Choices: ['Dove soap', 'Dove candy bar', 'Dove symbol', 'A dove (bird)']\n",
    "{\"Answer\": \"Dove symbol\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Question: A new compound is synthesized and found to be a monoprotic acid with a molar mass of 248 g/mol. When 0.0050 mol of this acid are dissolved in 0.500 L of water, the pH is measured as 3.89. What is the pKa of this acid?\n",
    "Choices: ['3.89', '7.78', '5.78', '2.33']\n",
    "{\"Answer\": 5.78}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Question: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I really loved my newborn baby boy. Scenario 2 | The new girl is embarrassed to live in a trailer park, so I invite the class to her house.\n",
    "Choices: ['Wrong, Wrong', 'Wrong, Not wrong', 'Not wrong, Wrong', 'Not wrong, Not wrong']\n",
    "{\"Answer\": \"Not wrong, Wrong\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Question: As of 2015, since 1990 forests have ____ in Europe and have ____ in Africa and the Americas.\n",
    "Choices:  ['increased, increased', 'increased, decreased', 'decreased, increased', 'decreased, decreased']\n",
    "{\"Answer\": \"increased, decreased\"}\n",
    "\n",
    "--------------------------\n",
    "\n",
    "Question: In the program below, the initial value of x is 5 and the initial value of y is 10.\n",
    " IF (X < O)\n",
    " {\n",
    " DISPLAY (\"Foxtrot\")\n",
    " }\n",
    " ELSE\n",
    " {\n",
    " IF (X > y)\n",
    " {\n",
    "   DISPLAY (\"Hotel\")\n",
    " }\n",
    " ELSE \n",
    " {\n",
    "   IF (y > O)\n",
    "   {\n",
    "   DISPLAY (\"November\")\n",
    "   }\n",
    "   ELSE\n",
    "   {\n",
    "   DISPLAY (\"Yankee\")\n",
    "   }\n",
    " }\n",
    " }\n",
    "What is displayed as a result of running the program?\n",
    "Choices: ['Foxtrot', 'Hotel', 'November', 'Yankee']\n",
    "{\"Answer\": \"November\"}\n",
    "\n",
    "After here, you will be given questions. Select the best answer from the choices provided. Do not explain your answer. Format your answer as with the examples above.\n",
    "'''\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f8812967484b5eb072f70da28263e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing evaluation set:   0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 0 Correct Answers\n",
      "20 / 100 Correct Answers\n",
      "64 / 200 Correct Answers\n",
      "105 / 300 Correct Answers\n",
      "143 / 400 Correct Answers\n",
      "185 / 500 Correct Answers\n",
      "218 / 600 Correct Answers\n",
      "262 / 700 Correct Answers\n",
      "306 / 800 Correct Answers\n",
      "339 / 900 Correct Answers\n",
      "364 / 1000 Correct Answers\n",
      "425 / 1200 Correct Answers\n",
      "462 / 1300 Correct Answers\n",
      "494 / 1400 Correct Answers\n",
      "524 / 1500 Correct Answers\n",
      "569 / 1600 Correct Answers\n",
      "608 / 1700 Correct Answers\n",
      "645 / 1800 Correct Answers\n",
      "665 / 1900 Correct Answers\n",
      "708 / 2000 Correct Answers\n",
      "753 / 2100 Correct Answers\n",
      "786 / 2200 Correct Answers\n",
      "825 / 2300 Correct Answers\n",
      "854 / 2400 Correct Answers\n",
      "885 / 2500 Correct Answers\n",
      "915 / 2600 Correct Answers\n",
      "956 / 2700 Correct Answers\n",
      "1000 / 2800 Correct Answers\n",
      "1080 / 3000 Correct Answers\n",
      "1147 / 3200 Correct Answers\n",
      "1185 / 3300 Correct Answers\n",
      "1227 / 3400 Correct Answers\n",
      "1268 / 3500 Correct Answers\n",
      "1310 / 3600 Correct Answers\n",
      "1351 / 3700 Correct Answers\n",
      "1398 / 3800 Correct Answers\n",
      "1432 / 3900 Correct Answers\n",
      "1454 / 4000 Correct Answers\n",
      "1480 / 4100 Correct Answers\n",
      "1510 / 4200 Correct Answers\n",
      "1538 / 4300 Correct Answers\n",
      "1566 / 4400 Correct Answers\n",
      "1602 / 4500 Correct Answers\n",
      "1642 / 4600 Correct Answers\n",
      "1680 / 4700 Correct Answers\n",
      "1709 / 4800 Correct Answers\n",
      "1849 / 5100 Correct Answers\n",
      "1898 / 5200 Correct Answers\n",
      "1950 / 5300 Correct Answers\n",
      "1995 / 5400 Correct Answers\n",
      "2025 / 5500 Correct Answers\n",
      "2087 / 5700 Correct Answers\n",
      "2129 / 5800 Correct Answers\n",
      "2170 / 5900 Correct Answers\n",
      "2212 / 6000 Correct Answers\n",
      "2253 / 6100 Correct Answers\n",
      "2299 / 6200 Correct Answers\n",
      "2338 / 6300 Correct Answers\n",
      "2381 / 6400 Correct Answers\n",
      "2416 / 6500 Correct Answers\n",
      "2456 / 6600 Correct Answers\n",
      "2491 / 6700 Correct Answers\n",
      "2512 / 6800 Correct Answers\n",
      "2546 / 6900 Correct Answers\n",
      "2590 / 7000 Correct Answers\n",
      "2644 / 7100 Correct Answers\n",
      "2709 / 7200 Correct Answers\n",
      "2761 / 7300 Correct Answers\n",
      "2806 / 7400 Correct Answers\n",
      "2912 / 7600 Correct Answers\n",
      "2959 / 7700 Correct Answers\n",
      "3011 / 7800 Correct Answers\n",
      "3070 / 7900 Correct Answers\n",
      "3128 / 8000 Correct Answers\n",
      "3187 / 8100 Correct Answers\n",
      "3235 / 8200 Correct Answers\n",
      "3280 / 8300 Correct Answers\n",
      "3325 / 8400 Correct Answers\n",
      "3356 / 8500 Correct Answers\n",
      "3378 / 8600 Correct Answers\n",
      "3402 / 8700 Correct Answers\n",
      "3428 / 8800 Correct Answers\n",
      "3454 / 8900 Correct Answers\n",
      "3480 / 9000 Correct Answers\n",
      "3508 / 9100 Correct Answers\n",
      "3531 / 9200 Correct Answers\n",
      "3552 / 9300 Correct Answers\n",
      "3586 / 9400 Correct Answers\n",
      "3623 / 9500 Correct Answers\n",
      "3666 / 9600 Correct Answers\n",
      "3703 / 9700 Correct Answers\n",
      "3742 / 9800 Correct Answers\n",
      "3781 / 9900 Correct Answers\n",
      "3822 / 10000 Correct Answers\n",
      "3855 / 10100 Correct Answers\n",
      "3894 / 10200 Correct Answers\n",
      "3943 / 10300 Correct Answers\n",
      "3981 / 10400 Correct Answers\n",
      "4013 / 10500 Correct Answers\n",
      "4046 / 10600 Correct Answers\n",
      "4063 / 10700 Correct Answers\n",
      "4082 / 10800 Correct Answers\n",
      "4101 / 10900 Correct Answers\n",
      "4121 / 11000 Correct Answers\n",
      "4142 / 11100 Correct Answers\n",
      "4163 / 11200 Correct Answers\n",
      "4180 / 11300 Correct Answers\n",
      "4197 / 11400 Correct Answers\n",
      "4224 / 11500 Correct Answers\n",
      "4246 / 11600 Correct Answers\n",
      "4265 / 11700 Correct Answers\n",
      "4287 / 11800 Correct Answers\n",
      "4308 / 11900 Correct Answers\n",
      "4322 / 12000 Correct Answers\n",
      "4347 / 12100 Correct Answers\n",
      "4375 / 12200 Correct Answers\n",
      "4411 / 12300 Correct Answers\n",
      "4444 / 12400 Correct Answers\n",
      "4476 / 12500 Correct Answers\n",
      "4514 / 12600 Correct Answers\n",
      "4543 / 12700 Correct Answers\n",
      "4577 / 12800 Correct Answers\n",
      "4606 / 12900 Correct Answers\n",
      "4641 / 13000 Correct Answers\n",
      "4675 / 13100 Correct Answers\n",
      "4722 / 13200 Correct Answers\n",
      "4765 / 13300 Correct Answers\n",
      "4802 / 13400 Correct Answers\n",
      "4847 / 13500 Correct Answers\n",
      "4890 / 13600 Correct Answers\n",
      "4942 / 13700 Correct Answers\n",
      "4981 / 13800 Correct Answers\n",
      "5028 / 13900 Correct Answers\n",
      "5076 / 14000 Correct Answers\n",
      "Results saved to llama_mmlu_responses_run6.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Processing the evaluation set with tqdm for a progress bar\n",
    "rows = []\n",
    "is_correct_count = 0\n",
    "output_file = 'llama_mmlu_responses_run5.csv'\n",
    "\n",
    "for idx in tqdm(range(0, len(dataset)), desc=\"Processing evaluation set\"):\n",
    "    format_error = 'None'\n",
    "    example = dataset[idx]\n",
    "\n",
    "    # Prepare the user question with few-shot prompt\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']\n",
    "    \n",
    "    stringified_choices = re.sub('\"', '\\'', str(choices))\n",
    "    # Append the new question to the few-shot prompt\n",
    "    user_question = f\"Question: {question}\\nChoices: {stringified_choices}\\nWhat is your final answer?\"\n",
    "    \n",
    "    # Messages to be passed to the model\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': few_shot_prompt},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    \n",
    "    # Run the model with torch.cuda.amp for mixed precision (parallel processing)\n",
    "    outputs = pipe(messages, max_new_tokens=64, temperature=0.1)\n",
    "    \n",
    "    output = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    # Extract LLaMA's answer (post-processed)\n",
    "    llama_answer = output.lower()\n",
    "    try:\n",
    "        llama_answer = json.loads(llama_answer)['answer']\n",
    "    except Exception as e:\n",
    "        format_error = 'Answer is not in JSON format.'\n",
    "        is_correct = choices[correct_answer].lower() in re.sub('\"', '\\'', llama_answer).lower()\n",
    "        is_correct_count += 1 if is_correct else 0\n",
    "        rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, format_error])\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Check if the answer matches the correct answer\n",
    "    if type(llama_answer) == str:\n",
    "        is_correct = choices[correct_answer].lower() == re.sub('\"', '\\'', llama_answer).lower()\n",
    "    else:\n",
    "        is_correct = False\n",
    "        format_error = 'Parsed answer is still dictionary.'\n",
    "\n",
    "    is_correct_count += 1 if is_correct else 0\n",
    "    if idx % 100 == 0:\n",
    "        print(f'{is_correct_count} / {idx} Correct Answers')\n",
    "    # Collect the result row\n",
    "    rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, format_error])\n",
    "\n",
    "# Write the collected results to a CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\", \"Format Error\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 36.31249109813417\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run5.csv')\n",
    "\n",
    "print(f'Average: {float(df[\"Is Correct\"].mean()) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 6: Compartmentalized prompting\n",
    "\n",
    "In this run, prompt shots are dependent on the subject of the current question being asked. In all prior runs, prompts were dependent on randomly nominated examples of questions and answers regardless of the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Ensure GPU is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize your LLaMA model pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for faster GPU performance\n",
    "    device=0,  # Use the first GPU\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "categories = train_dataset.unique('subject')\n",
    "# print(len(categories))\n",
    "items_compartmentalized = {}\n",
    "for c in categories:\n",
    "    items_compartmentalized[c] = {'shots': train_dataset.filter(lambda example: example['subject'] == c)}\n",
    "\n",
    "few_shot_prompt = \"You are a bot that's trained to answer questions in JSON format. For example:\\n\\n\"\n",
    "\n",
    "for c in categories:\n",
    "    idx = 0\n",
    "    per_subject_prompt = '' + few_shot_prompt\n",
    "    for item in items_compartmentalized[c]['shots']:\n",
    "        question = item['question']\n",
    "        choices = item['choices']\n",
    "        answer_index = item['answer']\n",
    "        actual_answer = choices[answer_index]\n",
    "        json_answer = {\"Answer\": f\"{actual_answer}\"}\n",
    "        stringified_json_answer = re.sub('\\'', '\"', str(json_answer))\n",
    "        item_prompt = f'Question: {question}\\nChoices: {choices}\\n{stringified_json_answer}'\n",
    "        per_subject_prompt = per_subject_prompt + item_prompt\n",
    "        if idx < 4:\n",
    "            per_subject_prompt += '\\n' + ('-' * 25) + '\\n'\n",
    "        idx += 1\n",
    "    items_compartmentalized[c]['prompt'] = per_subject_prompt + \"\\n\\nAfter here, you will be given questions. Select the best answer from the choices provided. Do not explain your answer. Format your answer as with the examples above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a bot that's trained to answer questions in JSON format. For example:\n",
      "\n",
      "Question: Find all c in Z_3 such that Z_3[x]/(x^2 + c) is a field.\n",
      "Choices: ['0', '1', '2', '3']\n",
      "{\"Answer\": \"1\"}\n",
      "-------------------------\n",
      "Question: Statement 1 | If aH is an element of a factor group, then |aH| divides |a|. Statement 2 | If H and K are subgroups of G then HK is a subgroup of G.\n",
      "Choices: ['True, True', 'False, False', 'True, False', 'False, True']\n",
      "{\"Answer\": \"False, False\"}\n",
      "-------------------------\n",
      "Question: Statement 1 | Every element of a group generates a cyclic subgroup of the group. Statement 2 | The symmetric group S_10 has 10 elements.\n",
      "Choices: ['True, True', 'False, False', 'True, False', 'False, True']\n",
      "{\"Answer\": \"True, False\"}\n",
      "-------------------------\n",
      "Question: Statement 1| Every function from a finite set onto itself must be one to one. Statement 2 | Every subgroup of an abelian group is abelian.\n",
      "Choices: ['True, True', 'False, False', 'True, False', 'False, True']\n",
      "{\"Answer\": \"True, True\"}\n",
      "-------------------------\n",
      "Question: Find the characteristic of the ring 2Z.\n",
      "Choices: ['0', '3', '12', '30']\n",
      "{\"Answer\": \"0\"}\n",
      "\n",
      "After here, you will be given questions. Select the best answer from the choices provided. Do not explain your answer. Format your answer as with the examples above.\n"
     ]
    }
   ],
   "source": [
    "print(items_compartmentalized['abstract_algebra']['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730b9ee0d9064e4aa8b91e3a427b3b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing evaluation set:   0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 0 Correct Answers\n",
      "18 / 100 Correct Answers\n",
      "62 / 200 Correct Answers\n",
      "99 / 300 Correct Answers\n",
      "138 / 400 Correct Answers\n",
      "189 / 500 Correct Answers\n",
      "220 / 600 Correct Answers\n",
      "261 / 700 Correct Answers\n",
      "305 / 800 Correct Answers\n",
      "347 / 900 Correct Answers\n",
      "377 / 1000 Correct Answers\n",
      "408 / 1100 Correct Answers\n",
      "436 / 1200 Correct Answers\n",
      "473 / 1300 Correct Answers\n",
      "510 / 1400 Correct Answers\n",
      "541 / 1500 Correct Answers\n",
      "595 / 1600 Correct Answers\n",
      "638 / 1700 Correct Answers\n",
      "679 / 1800 Correct Answers\n",
      "706 / 1900 Correct Answers\n",
      "745 / 2000 Correct Answers\n",
      "786 / 2100 Correct Answers\n",
      "818 / 2200 Correct Answers\n",
      "849 / 2300 Correct Answers\n",
      "879 / 2400 Correct Answers\n",
      "911 / 2500 Correct Answers\n",
      "944 / 2600 Correct Answers\n",
      "988 / 2700 Correct Answers\n",
      "1030 / 2800 Correct Answers\n",
      "1074 / 2900 Correct Answers\n",
      "1111 / 3000 Correct Answers\n",
      "1142 / 3100 Correct Answers\n",
      "1185 / 3200 Correct Answers\n",
      "1230 / 3300 Correct Answers\n",
      "1266 / 3400 Correct Answers\n",
      "1305 / 3500 Correct Answers\n",
      "1351 / 3600 Correct Answers\n",
      "1396 / 3700 Correct Answers\n",
      "1441 / 3800 Correct Answers\n",
      "1483 / 3900 Correct Answers\n",
      "1517 / 4000 Correct Answers\n",
      "1552 / 4100 Correct Answers\n",
      "1578 / 4200 Correct Answers\n",
      "1607 / 4300 Correct Answers\n",
      "1644 / 4400 Correct Answers\n",
      "1681 / 4500 Correct Answers\n",
      "1721 / 4600 Correct Answers\n",
      "1766 / 4700 Correct Answers\n",
      "1793 / 4800 Correct Answers\n",
      "1812 / 4900 Correct Answers\n",
      "1848 / 5000 Correct Answers\n",
      "1891 / 5100 Correct Answers\n",
      "1925 / 5200 Correct Answers\n",
      "1966 / 5300 Correct Answers\n",
      "2003 / 5400 Correct Answers\n",
      "2033 / 5500 Correct Answers\n",
      "2063 / 5600 Correct Answers\n",
      "2104 / 5700 Correct Answers\n",
      "2147 / 5800 Correct Answers\n",
      "2191 / 5900 Correct Answers\n",
      "2233 / 6000 Correct Answers\n",
      "2273 / 6100 Correct Answers\n",
      "2317 / 6200 Correct Answers\n",
      "2361 / 6300 Correct Answers\n",
      "2407 / 6400 Correct Answers\n",
      "2445 / 6500 Correct Answers\n",
      "2490 / 6600 Correct Answers\n",
      "2532 / 6700 Correct Answers\n",
      "2565 / 6800 Correct Answers\n",
      "2602 / 6900 Correct Answers\n",
      "2648 / 7000 Correct Answers\n",
      "2701 / 7100 Correct Answers\n",
      "2758 / 7200 Correct Answers\n",
      "2818 / 7300 Correct Answers\n",
      "2864 / 7400 Correct Answers\n",
      "2920 / 7500 Correct Answers\n",
      "2962 / 7600 Correct Answers\n",
      "3014 / 7700 Correct Answers\n",
      "3066 / 7800 Correct Answers\n",
      "3127 / 7900 Correct Answers\n",
      "3183 / 8000 Correct Answers\n",
      "3240 / 8100 Correct Answers\n",
      "3289 / 8200 Correct Answers\n",
      "3321 / 8300 Correct Answers\n",
      "3352 / 8400 Correct Answers\n",
      "3391 / 8500 Correct Answers\n",
      "3415 / 8600 Correct Answers\n",
      "3439 / 8700 Correct Answers\n",
      "3464 / 8800 Correct Answers\n",
      "3490 / 8900 Correct Answers\n",
      "3516 / 9000 Correct Answers\n",
      "3539 / 9100 Correct Answers\n",
      "3556 / 9200 Correct Answers\n",
      "3578 / 9300 Correct Answers\n",
      "3605 / 9400 Correct Answers\n",
      "3651 / 9500 Correct Answers\n",
      "3700 / 9600 Correct Answers\n",
      "3740 / 9700 Correct Answers\n",
      "3778 / 9800 Correct Answers\n",
      "3813 / 9900 Correct Answers\n",
      "3860 / 10000 Correct Answers\n",
      "3897 / 10100 Correct Answers\n",
      "3942 / 10200 Correct Answers\n",
      "3991 / 10300 Correct Answers\n",
      "4028 / 10400 Correct Answers\n",
      "4048 / 10500 Correct Answers\n",
      "4080 / 10600 Correct Answers\n",
      "4102 / 10700 Correct Answers\n",
      "4124 / 10800 Correct Answers\n",
      "4144 / 10900 Correct Answers\n",
      "4168 / 11000 Correct Answers\n",
      "4194 / 11100 Correct Answers\n",
      "4216 / 11200 Correct Answers\n",
      "4238 / 11300 Correct Answers\n",
      "4263 / 11400 Correct Answers\n",
      "4292 / 11500 Correct Answers\n",
      "4315 / 11600 Correct Answers\n",
      "4334 / 11700 Correct Answers\n",
      "4354 / 11800 Correct Answers\n",
      "4372 / 11900 Correct Answers\n",
      "4394 / 12000 Correct Answers\n",
      "4417 / 12100 Correct Answers\n",
      "4446 / 12200 Correct Answers\n",
      "4487 / 12300 Correct Answers\n",
      "4520 / 12400 Correct Answers\n",
      "4554 / 12500 Correct Answers\n",
      "4595 / 12600 Correct Answers\n",
      "4632 / 12700 Correct Answers\n",
      "4668 / 12800 Correct Answers\n",
      "4707 / 12900 Correct Answers\n",
      "4749 / 13000 Correct Answers\n",
      "4787 / 13100 Correct Answers\n",
      "4833 / 13200 Correct Answers\n",
      "4881 / 13300 Correct Answers\n",
      "4922 / 13400 Correct Answers\n",
      "4957 / 13500 Correct Answers\n",
      "5003 / 13600 Correct Answers\n",
      "5052 / 13700 Correct Answers\n",
      "5091 / 13800 Correct Answers\n",
      "5136 / 13900 Correct Answers\n",
      "5185 / 14000 Correct Answers\n",
      "Results saved to llama_mmlu_responses_run7.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "rows = []\n",
    "is_correct_count = 0\n",
    "output_file = 'llama_mmlu_responses_run7.csv'\n",
    "\n",
    "for idx in tqdm(range(0, len(dataset)), desc=\"Processing evaluation set\"):\n",
    "    format_error = 'None'\n",
    "    example = dataset[idx]\n",
    "\n",
    "    # Prepare the user question with few-shot prompt\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    correct_answer = example['answer']\n",
    "    subject = example['subject']\n",
    "    \n",
    "    stringified_choices = re.sub('\"', '\\'', str(choices))\n",
    "    # Append the new question to the few-shot prompt\n",
    "    user_question = f\"Question: {question}\\nChoices: {stringified_choices}\\nWhat is your final answer?\"\n",
    "    \n",
    "    # Messages to be passed to the model\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': items_compartmentalized[subject]['prompt']},\n",
    "        {'role': 'user', 'content': user_question},\n",
    "    ]\n",
    "    \n",
    "    outputs = pipe(messages, max_new_tokens=20, temperature=0.1)\n",
    "    \n",
    "    output = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    # Extract LLaMA's answer (post-processed)\n",
    "    llama_answer = output.lower()\n",
    "    try:\n",
    "        llama_answer = json.loads(llama_answer)['answer']\n",
    "    except Exception as e:\n",
    "        format_error = 'Answer is not in JSON format.'\n",
    "        is_correct = choices[correct_answer].lower() in re.sub('\"', '\\'', llama_answer).lower()\n",
    "        is_correct_count += 1 if is_correct else 0\n",
    "        rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, subject, format_error])\n",
    "        if idx % 100 == 0:\n",
    "            print(f'{is_correct_count} / {idx} Correct Answers')\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Check if the answer matches the correct answer\n",
    "    if type(llama_answer) == str:\n",
    "        is_correct = choices[correct_answer].lower() == re.sub('\"', '\\'', llama_answer).lower()\n",
    "    else:\n",
    "        is_correct = False\n",
    "        format_error = 'Parsed answer is still dictionary.'\n",
    "\n",
    "    is_correct_count += 1 if is_correct else 0\n",
    "    if idx % 100 == 0:\n",
    "        print(f'{is_correct_count} / {idx} Correct Answers')\n",
    "    # Collect the result row\n",
    "    rows.append([question, choices, llama_answer, choices[correct_answer], is_correct, subject, format_error])\n",
    "\n",
    "# Write the collected results to a CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\", \"Subject\", \"Format Error\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Subject  Is Correct\n",
      "0                      abstract_algebra    0.180000\n",
      "1                               anatomy    0.459259\n",
      "2                             astronomy    0.355263\n",
      "3                       business_ethics    0.490000\n",
      "4                    clinical_knowledge    0.381132\n",
      "5                       college_biology    0.416667\n",
      "6                     college_chemistry    0.330000\n",
      "7              college_computer_science    0.300000\n",
      "8                   college_mathematics    0.270000\n",
      "9                      college_medicine    0.404624\n",
      "10                      college_physics    0.225490\n",
      "11                    computer_security    0.550000\n",
      "12                   conceptual_physics    0.417021\n",
      "13                         econometrics    0.289474\n",
      "14               electrical_engineering    0.434483\n",
      "15               elementary_mathematics    0.309524\n",
      "16                         formal_logic    0.309524\n",
      "17                         global_facts    0.380000\n",
      "18                  high_school_biology    0.425806\n",
      "19                high_school_chemistry    0.369458\n",
      "20         high_school_computer_science    0.440000\n",
      "21         high_school_european_history    0.375758\n",
      "22                high_school_geography    0.444444\n",
      "23  high_school_government_and_politics    0.466321\n",
      "24           high_school_macroeconomics    0.325641\n",
      "25              high_school_mathematics    0.333333\n",
      "26           high_school_microeconomics    0.415966\n",
      "27                  high_school_physics    0.198675\n",
      "28               high_school_psychology    0.385321\n",
      "29               high_school_statistics    0.300926\n",
      "30               high_school_us_history    0.411765\n",
      "31            high_school_world_history    0.434599\n",
      "32                          human_aging    0.434978\n",
      "33                      human_sexuality    0.419847\n",
      "34                    international_law    0.404959\n",
      "35                        jurisprudence    0.462963\n",
      "36                    logical_fallacies    0.361963\n",
      "37                     machine_learning    0.383929\n",
      "38                           management    0.456311\n",
      "39                            marketing    0.589744\n",
      "40                     medical_genetics    0.490000\n",
      "41                        miscellaneous    0.544061\n",
      "42                       moral_disputes    0.323699\n",
      "43                      moral_scenarios    0.236872\n",
      "44                            nutrition    0.454248\n",
      "45                           philosophy    0.382637\n",
      "46                           prehistory    0.459877\n",
      "47              professional_accounting    0.255319\n",
      "48                     professional_law    0.224902\n",
      "49                professional_medicine    0.382353\n",
      "50              professional_psychology    0.377451\n",
      "51                     public_relations    0.481818\n",
      "52                     security_studies    0.424490\n",
      "53                            sociology    0.412935\n",
      "54                    us_foreign_policy    0.490000\n",
      "55                             virology    0.397590\n",
      "56                      world_religions    0.520468\n",
      "Macro Accuracy 38.95419141766825\n",
      "Micro Accuracy: 37.09585529126905\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run6.csv')\n",
    "class_accuracies = df.groupby('Subject')['Is Correct'].mean().reset_index()\n",
    "print(class_accuracies)\n",
    "\n",
    "macro_accuracy = class_accuracies['Is Correct'].mean()\n",
    "print('Macro Accuracy', macro_accuracy * 100)\n",
    "print(f'Micro Accuracy: {float(df[\"Is Correct\"].mean()) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "print(len(items_compartmentalized.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 7: Use Prompts from Official LLaMA Eval\n",
    "\n",
    "From their [eval details](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md),\n",
    "\n",
    "> Macro averages are reported unless otherwise stated. The micro average scores for the various models are: 65.6, 79.0, and 85.4 for the pre-trained 8B, 70B and 405B models respectively for the 5-shot config; 69.44, 84.0, 87.71 for the post-trained 8B, 70B and 405B models respectively for the 5-shot config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_type                                 14042\n",
      "task_name                                 14042\n",
      "subtask_name                              14042\n",
      "input_question                            14042\n",
      "input_choice_list                         14042\n",
      "input_final_prompts                       14042\n",
      "input_correct_responses                   14042\n",
      "output_prediction_text                    14042\n",
      "output_parsed_answer                      14042\n",
      "output_choice_completions                     0\n",
      "output_choice_negative_log_likelihoods        0\n",
      "output_metrics                            14042\n",
      "is_correct                                14042\n",
      "input_question_hash                       14042\n",
      "input_final_prompts_hash                  14042\n",
      "benchmark_label                           14042\n",
      "eval_config                               14042\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load your dataset and set up the model pipeline\n",
    "# output_file = 'llama_mmlu_responses_5shot.csv'\n",
    "\n",
    "# Ensure GPU is used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize your LLaMA model pipeline\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_path,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for faster GPU performance\n",
    "    device=0,  # Use the first GPU\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# These are the details coming from HuggingFace evaluators\n",
    "# https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals/tree/main/Llama-3.2-1B-Instruct-evals\n",
    "data = pd.read_parquet(\"Details_mmlu_2024-09-23T17-23-07.347337.parquet.gzip\")\n",
    "print(data.count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the optimized settings mentioned by the evaluators for the official eval. https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals/tree/main/Llama-3.2-1B-Instruct-evals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_p': '0',\n",
       " 'seed': '42',\n",
       " 'num_generations': '1',\n",
       " 'max_gen_len': '10',\n",
       " 'top_k': '0',\n",
       " 'temperature': '0.0',\n",
       " 'prompt_fn': 'functools.partial(<function jinja_dialog_format at 0x7f02e368d090>, template={\\'prompt\\': \\'Given the following question and four candidate answers (A, B, C and D), choose the best answer.\\\\nQuestion: {{ question }}\\\\nA. {{ choices[\"A\"] }}\\\\nB. {{ choices[\"B\"] }}\\\\nC. {{ choices[\"C\"] }}\\\\nD. {{ choices[\"D\"] }}\\\\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.\\\\n\\', \\'answer\\': \\'The best answer is {{ answer }}.\\', \\'gen_prefix\\': \\'The best answer is \\'}, append_gen_prefix=True)',\n",
       " 'num_few_shot': '5',\n",
       " 'max_prompt_len': '3840',\n",
       " 'return_logprobs': 'false'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This file can be found in: https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals/tree/main/Llama-3.2-1B-Instruct-evals\n",
    "metrics = pd.read_parquet('Llama-3.2-1B-Instruct-evals_Details_metrics_details_2024-09-23T17-23-22.207810.parquet.gzip')\n",
    "metrics[metrics['benchmark_label'] == 'MMLU'].iloc[0]['eval_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\n",
      "Question: The energy for all forms of muscle contraction is provided by:\n",
      "A. ATP.\n",
      "B. ADP.\n",
      "C. phosphocreatine.\n",
      "D. oxidative phosphorylation.\n",
      "Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The best answer is A.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\n",
      "Question: What is the difference between a male and a female catheter?\n",
      "A. Male and female catheters are different colours.\n",
      "B. Male catheters are longer than female catheters.\n",
      "C. Male catheters are bigger than female catheters.\n",
      "D. Female catheters are longer than male catheters.\n",
      "Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The best answer is B.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\n",
      "Question: In the assessment of the hand function which of the following is true?\n",
      "A. Abduction of the thumb is supplied by spinal root T2\n",
      "B. Opposition of the thumb by opponens policis is supplied by spinal root T1\n",
      "C. Finger adduction is supplied by the median nerve\n",
      "D. Finger abduction is mediated by the palmar interossei\n",
      "Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The best answer is B.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\n",
      "Question: How many attempts should you make to cannulate a patient before passing the job on to a senior colleague, according to the medical knowledge of 2020?\n",
      "A. 4\n",
      "B. 3\n",
      "C. 2\n",
      "D. 1\n",
      "Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The best answer is C.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\n",
      "Question: Glycolysis is the name given to the pathway involving the conversion of:\n",
      "A. glycogen to glucose-1-phosphate.\n",
      "B. glycogen or glucose to fructose.\n",
      "C. glycogen or glucose to pyruvate or lactate.\n",
      "D. glycogen or glucose to pyruvate or acetyl CoA.\n",
      "Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The best answer is C.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\n",
      "Question: Blood clots are responsible for repeatedly blocking a catheter. What should you do?\n",
      "A. Call for help.\n",
      "B. Inform medical staff.\n",
      "C. Inform the patient.\n",
      "D. Remove the catheter and recatheterize.\n",
      "Your response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The best answer is\n"
     ]
    }
   ],
   "source": [
    "# Example prompt format\n",
    "print(data.iloc[0]['input_final_prompts'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0cbb88cc7a4039826b1a0ecc6e8ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/students/juan/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/students/juan/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/data/students/juan/anaconda3/envs/dl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 0 Correct Answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 / 100 Correct Answers\n",
      "106 / 200 Correct Answers\n",
      "157 / 300 Correct Answers\n",
      "181 / 400 Correct Answers\n",
      "214 / 500 Correct Answers\n",
      "267 / 600 Correct Answers\n",
      "309 / 700 Correct Answers\n",
      "363 / 800 Correct Answers\n",
      "395 / 900 Correct Answers\n",
      "443 / 1000 Correct Answers\n",
      "491 / 1100 Correct Answers\n",
      "519 / 1200 Correct Answers\n",
      "534 / 1300 Correct Answers\n",
      "572 / 1400 Correct Answers\n",
      "599 / 1500 Correct Answers\n",
      "609 / 1600 Correct Answers\n",
      "647 / 1700 Correct Answers\n",
      "720 / 1800 Correct Answers\n",
      "754 / 1900 Correct Answers\n",
      "785 / 2000 Correct Answers\n",
      "827 / 2100 Correct Answers\n",
      "865 / 2200 Correct Answers\n",
      "910 / 2300 Correct Answers\n",
      "959 / 2400 Correct Answers\n",
      "997 / 2500 Correct Answers\n",
      "1026 / 2600 Correct Answers\n",
      "1063 / 2700 Correct Answers\n",
      "1091 / 2800 Correct Answers\n",
      "1147 / 2900 Correct Answers\n",
      "1189 / 3000 Correct Answers\n",
      "1225 / 3100 Correct Answers\n",
      "1258 / 3200 Correct Answers\n",
      "1314 / 3300 Correct Answers\n",
      "1382 / 3400 Correct Answers\n",
      "1448 / 3500 Correct Answers\n",
      "1503 / 3600 Correct Answers\n",
      "1553 / 3700 Correct Answers\n",
      "1595 / 3800 Correct Answers\n",
      "1661 / 3900 Correct Answers\n",
      "1730 / 4000 Correct Answers\n",
      "1788 / 4100 Correct Answers\n",
      "1843 / 4200 Correct Answers\n",
      "1885 / 4300 Correct Answers\n",
      "1922 / 4400 Correct Answers\n",
      "1973 / 4500 Correct Answers\n",
      "2022 / 4600 Correct Answers\n",
      "2071 / 4700 Correct Answers\n",
      "2118 / 4800 Correct Answers\n",
      "2171 / 4900 Correct Answers\n",
      "2212 / 5000 Correct Answers\n",
      "2265 / 5100 Correct Answers\n",
      "2322 / 5200 Correct Answers\n",
      "2364 / 5300 Correct Answers\n",
      "2405 / 5400 Correct Answers\n",
      "2440 / 5500 Correct Answers\n",
      "2491 / 5600 Correct Answers\n",
      "2537 / 5700 Correct Answers\n",
      "2553 / 5800 Correct Answers\n",
      "2606 / 5900 Correct Answers\n",
      "2661 / 6000 Correct Answers\n",
      "2716 / 6100 Correct Answers\n",
      "2774 / 6200 Correct Answers\n",
      "2828 / 6300 Correct Answers\n",
      "2874 / 6400 Correct Answers\n",
      "2911 / 6500 Correct Answers\n",
      "2973 / 6600 Correct Answers\n",
      "3040 / 6700 Correct Answers\n",
      "3090 / 6800 Correct Answers\n",
      "3143 / 6900 Correct Answers\n",
      "3190 / 7000 Correct Answers\n",
      "3212 / 7100 Correct Answers\n",
      "3242 / 7200 Correct Answers\n",
      "3305 / 7300 Correct Answers\n",
      "3360 / 7400 Correct Answers\n",
      "3400 / 7500 Correct Answers\n",
      "3448 / 7600 Correct Answers\n",
      "3508 / 7700 Correct Answers\n",
      "3546 / 7800 Correct Answers\n",
      "3579 / 7900 Correct Answers\n",
      "3631 / 8000 Correct Answers\n",
      "3681 / 8100 Correct Answers\n",
      "3726 / 8200 Correct Answers\n",
      "3752 / 8300 Correct Answers\n",
      "3816 / 8400 Correct Answers\n",
      "3878 / 8500 Correct Answers\n",
      "3918 / 8600 Correct Answers\n",
      "3952 / 8700 Correct Answers\n",
      "3997 / 8800 Correct Answers\n",
      "4035 / 8900 Correct Answers\n",
      "4068 / 9000 Correct Answers\n",
      "4107 / 9100 Correct Answers\n",
      "4138 / 9200 Correct Answers\n",
      "4171 / 9300 Correct Answers\n",
      "4190 / 9400 Correct Answers\n",
      "4205 / 9500 Correct Answers\n",
      "4237 / 9600 Correct Answers\n",
      "4272 / 9700 Correct Answers\n",
      "4311 / 9800 Correct Answers\n",
      "4338 / 9900 Correct Answers\n",
      "4367 / 10000 Correct Answers\n",
      "4386 / 10100 Correct Answers\n",
      "4436 / 10200 Correct Answers\n",
      "4496 / 10300 Correct Answers\n",
      "4548 / 10400 Correct Answers\n",
      "4560 / 10500 Correct Answers\n",
      "4579 / 10600 Correct Answers\n",
      "4631 / 10700 Correct Answers\n",
      "4679 / 10800 Correct Answers\n",
      "4717 / 10900 Correct Answers\n",
      "4753 / 11000 Correct Answers\n",
      "4787 / 11100 Correct Answers\n",
      "4822 / 11200 Correct Answers\n",
      "4854 / 11300 Correct Answers\n",
      "4905 / 11400 Correct Answers\n",
      "4954 / 11500 Correct Answers\n",
      "5012 / 11600 Correct Answers\n",
      "5053 / 11700 Correct Answers\n",
      "5093 / 11800 Correct Answers\n",
      "5129 / 11900 Correct Answers\n",
      "5143 / 12000 Correct Answers\n",
      "5170 / 12100 Correct Answers\n",
      "5216 / 12200 Correct Answers\n",
      "5262 / 12300 Correct Answers\n",
      "5302 / 12400 Correct Answers\n",
      "5337 / 12500 Correct Answers\n",
      "5383 / 12600 Correct Answers\n",
      "5429 / 12700 Correct Answers\n",
      "5482 / 12800 Correct Answers\n",
      "5530 / 12900 Correct Answers\n",
      "5567 / 13000 Correct Answers\n",
      "5611 / 13100 Correct Answers\n",
      "5633 / 13200 Correct Answers\n",
      "5656 / 13300 Correct Answers\n",
      "5703 / 13400 Correct Answers\n",
      "5757 / 13500 Correct Answers\n",
      "5801 / 13600 Correct Answers\n",
      "5832 / 13700 Correct Answers\n",
      "5887 / 13800 Correct Answers\n",
      "5945 / 13900 Correct Answers\n",
      "5999 / 14000 Correct Answers\n",
      "Results saved to llama_mmlu_responses_run7.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "output_file = f'llama_mmlu_responses_run7.csv'\n",
    "\n",
    "def parse_answer(answer_string):\n",
    "    # Check if the string starts with the expected phrase\n",
    "    expected_start = \"The best answer is \"\n",
    "    answer_string = re.sub(r'\\\\n', '', answer_string.strip())\n",
    "    \n",
    "    # Guard clause to handle cases where the string doesn't start as expected\n",
    "    if not answer_string.startswith(expected_start):\n",
    "        raise ValueError(f\"Invalid answer format: '{answer_string}' does not start with '{expected_start}'\")\n",
    "\n",
    "    # Parse the answer part (assuming it's a single character like 'A', 'B', etc.)\n",
    "    answer = answer_string[len(expected_start):len(expected_start)+1].strip()\n",
    "\n",
    "    # Additional check to ensure there's an answer after the phrase\n",
    "    if not answer or len(answer) > 1:\n",
    "        raise ValueError(f\"Invalid or missing answer choice in '{answer_string}'\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct_count = 0\n",
    "rows = []\n",
    "output_file = 'llama_mmlu_responses_run7.csv'\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    total += 1\n",
    "    item = data.iloc[i]\n",
    "    final_prompts = item['input_final_prompts']\n",
    "    correct_answer = item['input_correct_responses'][0]\n",
    "    question = item['input_question']\n",
    "    choices = item['input_choice_list']\n",
    "    subject = item['subtask_name']\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': final_prompts},\n",
    "    ]\n",
    "\n",
    "    outputs = pipe(messages, max_new_tokens=10, do_sample=False)\n",
    "    \n",
    "    output = outputs[0]['generated_text'][-1]['content'].strip()\n",
    "    llama_answer = ''\n",
    "\n",
    "    try:\n",
    "        llama_answer = parse_answer(output)\n",
    "    except ValueError:\n",
    "        llama_answer = f'Invalid string format: {output}'\n",
    "    \n",
    "    is_correct = False\n",
    "    if correct_answer == llama_answer:\n",
    "        correct_count += 1\n",
    "        is_correct = True\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'{correct_count} / {i} Correct Answers')\n",
    "    rows.append([question, choices, llama_answer, correct_answer, is_correct, subject])\n",
    "\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Question\", \"Choices\", \"LLaMA Answer\", \"Correct Answer\", \"Is Correct\", \"Subject\"])\n",
    "    # Write all rows at once\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Subject  Is Correct\n",
      "0                      mmlu_chat.abstract_algebra    0.290000\n",
      "1                               mmlu_chat.anatomy    0.496296\n",
      "2                             mmlu_chat.astronomy    0.559211\n",
      "3                       mmlu_chat.business_ethics    0.450000\n",
      "4                    mmlu_chat.clinical_knowledge    0.471698\n",
      "5                       mmlu_chat.college_biology    0.500000\n",
      "6                     mmlu_chat.college_chemistry    0.350000\n",
      "7              mmlu_chat.college_computer_science    0.330000\n",
      "8                   mmlu_chat.college_mathematics    0.330000\n",
      "9                      mmlu_chat.college_medicine    0.421965\n",
      "10                      mmlu_chat.college_physics    0.196078\n",
      "11                    mmlu_chat.computer_security    0.570000\n",
      "12                   mmlu_chat.conceptual_physics    0.400000\n",
      "13                         mmlu_chat.econometrics    0.263158\n",
      "14               mmlu_chat.electrical_engineering    0.565517\n",
      "15               mmlu_chat.elementary_mathematics    0.256614\n",
      "16                         mmlu_chat.formal_logic    0.126984\n",
      "17                         mmlu_chat.global_facts    0.340000\n",
      "18                  mmlu_chat.high_school_biology    0.532258\n",
      "19                mmlu_chat.high_school_chemistry    0.403941\n",
      "20         mmlu_chat.high_school_computer_science    0.300000\n",
      "21         mmlu_chat.high_school_european_history    0.151515\n",
      "22                mmlu_chat.high_school_geography    0.606061\n",
      "23  mmlu_chat.high_school_government_and_politics    0.569948\n",
      "24           mmlu_chat.high_school_macroeconomics    0.376923\n",
      "25              mmlu_chat.high_school_mathematics    0.229630\n",
      "26           mmlu_chat.high_school_microeconomics    0.407563\n",
      "27                  mmlu_chat.high_school_physics    0.344371\n",
      "28               mmlu_chat.high_school_psychology    0.653211\n",
      "29               mmlu_chat.high_school_statistics    0.333333\n",
      "30               mmlu_chat.high_school_us_history    0.122549\n",
      "31            mmlu_chat.high_school_world_history    0.345992\n",
      "32                          mmlu_chat.human_aging    0.520179\n",
      "33                      mmlu_chat.human_sexuality    0.503817\n",
      "34                    mmlu_chat.international_law    0.636364\n",
      "35                        mmlu_chat.jurisprudence    0.546296\n",
      "36                    mmlu_chat.logical_fallacies    0.453988\n",
      "37                     mmlu_chat.machine_learning    0.330357\n",
      "38                           mmlu_chat.management    0.592233\n",
      "39                            mmlu_chat.marketing    0.696581\n",
      "40                     mmlu_chat.medical_genetics    0.570000\n",
      "41                        mmlu_chat.miscellaneous    0.619413\n",
      "42                       mmlu_chat.moral_disputes    0.502890\n",
      "43                      mmlu_chat.moral_scenarios    0.153073\n",
      "44                            mmlu_chat.nutrition    0.542484\n",
      "45                           mmlu_chat.philosophy    0.533762\n",
      "46                           mmlu_chat.prehistory    0.558642\n",
      "47              mmlu_chat.professional_accounting    0.351064\n",
      "48                     mmlu_chat.professional_law    0.345502\n",
      "49                mmlu_chat.professional_medicine    0.551471\n",
      "50              mmlu_chat.professional_psychology    0.424837\n",
      "51                     mmlu_chat.public_relations    0.463636\n",
      "52                     mmlu_chat.security_studies    0.457143\n",
      "53                            mmlu_chat.sociology    0.577114\n",
      "54                    mmlu_chat.us_foreign_policy    0.670000\n",
      "55                             mmlu_chat.virology    0.421687\n",
      "56                      mmlu_chat.world_religions    0.584795\n",
      "Macro Accuracy 43.68797016622361\n",
      "Micro Accuracy: 42.91411479846175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run7.csv')\n",
    "class_accuracies = df.groupby('Subject')['Is Correct'].mean().reset_index()\n",
    "print(class_accuracies)\n",
    "\n",
    "macro_accuracy = class_accuracies['Is Correct'].mean()\n",
    "print('Macro Accuracy', macro_accuracy * 100)\n",
    "print(f'Micro Accuracy: {float(df[\"Is Correct\"].mean()) * 100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject\n",
      "mmlu_chat.abstract_algebra                       100.000000\n",
      "mmlu_chat.anatomy                                100.000000\n",
      "mmlu_chat.astronomy                               98.684211\n",
      "mmlu_chat.business_ethics                        100.000000\n",
      "mmlu_chat.clinical_knowledge                      95.094340\n",
      "mmlu_chat.college_biology                        100.000000\n",
      "mmlu_chat.college_chemistry                       99.000000\n",
      "mmlu_chat.college_computer_science               100.000000\n",
      "mmlu_chat.college_mathematics                    100.000000\n",
      "mmlu_chat.college_medicine                        98.265896\n",
      "mmlu_chat.college_physics                        100.000000\n",
      "mmlu_chat.computer_security                       99.000000\n",
      "mmlu_chat.conceptual_physics                     100.000000\n",
      "mmlu_chat.econometrics                           100.000000\n",
      "mmlu_chat.electrical_engineering                 100.000000\n",
      "mmlu_chat.elementary_mathematics                 100.000000\n",
      "mmlu_chat.formal_logic                            40.476190\n",
      "mmlu_chat.global_facts                           100.000000\n",
      "mmlu_chat.high_school_biology                    100.000000\n",
      "mmlu_chat.high_school_chemistry                   98.522167\n",
      "mmlu_chat.high_school_computer_science            91.000000\n",
      "mmlu_chat.high_school_european_history            28.484848\n",
      "mmlu_chat.high_school_geography                  100.000000\n",
      "mmlu_chat.high_school_government_and_politics    100.000000\n",
      "mmlu_chat.high_school_macroeconomics             100.000000\n",
      "mmlu_chat.high_school_mathematics                 82.222222\n",
      "mmlu_chat.high_school_microeconomics             100.000000\n",
      "mmlu_chat.high_school_physics                    100.000000\n",
      "mmlu_chat.high_school_psychology                 100.000000\n",
      "mmlu_chat.high_school_statistics                 100.000000\n",
      "mmlu_chat.high_school_us_history                  27.450980\n",
      "mmlu_chat.high_school_world_history               55.696203\n",
      "mmlu_chat.human_aging                             98.654709\n",
      "mmlu_chat.human_sexuality                         93.129771\n",
      "mmlu_chat.international_law                      100.000000\n",
      "mmlu_chat.jurisprudence                          100.000000\n",
      "mmlu_chat.logical_fallacies                      100.000000\n",
      "mmlu_chat.machine_learning                        98.214286\n",
      "mmlu_chat.management                             100.000000\n",
      "mmlu_chat.marketing                              100.000000\n",
      "mmlu_chat.medical_genetics                       100.000000\n",
      "mmlu_chat.miscellaneous                           97.828863\n",
      "mmlu_chat.moral_disputes                         100.000000\n",
      "mmlu_chat.moral_scenarios                         76.201117\n",
      "mmlu_chat.nutrition                              100.000000\n",
      "mmlu_chat.philosophy                             100.000000\n",
      "mmlu_chat.prehistory                              98.456790\n",
      "mmlu_chat.professional_accounting                 99.645390\n",
      "mmlu_chat.professional_law                       100.000000\n",
      "mmlu_chat.professional_medicine                  100.000000\n",
      "mmlu_chat.professional_psychology                100.000000\n",
      "mmlu_chat.public_relations                       100.000000\n",
      "mmlu_chat.security_studies                        89.387755\n",
      "mmlu_chat.sociology                               99.502488\n",
      "mmlu_chat.us_foreign_policy                      100.000000\n",
      "mmlu_chat.virology                                99.397590\n",
      "mmlu_chat.world_religions                         84.210526\n",
      "Name: contains_phrase, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "phrase = 'Invalid string format'\n",
    "df['contains_phrase'] = df['LLaMA Answer'].str.contains(phrase, case=False, na=False)\n",
    "grouped = df.groupby('Subject')\n",
    "percentages = ((grouped['contains_phrase'].count() - grouped['contains_phrase'].sum()) / grouped['contains_phrase'].count()) * 100\n",
    "\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2056250/3806437871.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metrics_df['contains_phrase'] = metrics_df['metric_tag'].str.contains(phrase, case=False, na=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_tag</th>\n",
       "      <th>metric_value_computed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>mmlu_chat.world_religions/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>mmlu_chat.virology/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>mmlu_chat.us_foreign_policy/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>mmlu_chat.sociology/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>mmlu_chat.security_studies/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>mmlu_chat.public_relations/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>mmlu_chat.professional_psychology/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>mmlu_chat.professional_medicine/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>mmlu_chat.professional_law/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>mmlu_chat.professional_accounting/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>mmlu_chat.prehistory/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>mmlu_chat.philosophy/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>mmlu_chat.nutrition/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>mmlu_chat.moral_scenarios/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>mmlu_chat.moral_disputes/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>mmlu_chat.miscellaneous/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>mmlu_chat.medical_genetics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>mmlu_chat.marketing/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>mmlu_chat.management/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>mmlu_chat.machine_learning/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>mmlu_chat.logical_fallacies/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>mmlu_chat.jurisprudence/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>mmlu_chat.international_law/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>mmlu_chat.human_sexuality/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>mmlu_chat.human_aging/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>mmlu_chat.high_school_world_history/correct_fo...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>mmlu_chat.high_school_us_history/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>mmlu_chat.high_school_statistics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>mmlu_chat.high_school_psychology/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>mmlu_chat.high_school_physics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>mmlu_chat.high_school_microeconomics/correct_f...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>mmlu_chat.high_school_mathematics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>mmlu_chat.high_school_macroeconomics/correct_f...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>mmlu_chat.high_school_government_and_politics/...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>mmlu_chat.high_school_geography/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>mmlu_chat.high_school_european_history/correct...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>mmlu_chat.high_school_computer_science/correct...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>mmlu_chat.high_school_chemistry/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>mmlu_chat.high_school_biology/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>mmlu_chat.global_facts/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>mmlu_chat.formal_logic/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>mmlu_chat.elementary_mathematics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>mmlu_chat.electrical_engineering/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>mmlu_chat.econometrics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>mmlu_chat.conceptual_physics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>mmlu_chat.computer_security/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>mmlu_chat.college_physics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>mmlu_chat.college_medicine/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>mmlu_chat.college_mathematics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>mmlu_chat.college_computer_science/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>mmlu_chat.college_chemistry/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>mmlu_chat.college_biology/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>mmlu_chat.clinical_knowledge/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>mmlu_chat.business_ethics/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>mmlu_chat.astronomy/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>mmlu_chat.anatomy/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>mmlu_chat.abstract_algebra/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>micro_avg/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>macro_avg/correct_format</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             metric_tag  metric_value_computed\n",
       "890            mmlu_chat.world_religions/correct_format                  100.0\n",
       "892                   mmlu_chat.virology/correct_format                  100.0\n",
       "894          mmlu_chat.us_foreign_policy/correct_format                  100.0\n",
       "896                  mmlu_chat.sociology/correct_format                  100.0\n",
       "898           mmlu_chat.security_studies/correct_format                  100.0\n",
       "900           mmlu_chat.public_relations/correct_format                  100.0\n",
       "902    mmlu_chat.professional_psychology/correct_format                  100.0\n",
       "904      mmlu_chat.professional_medicine/correct_format                  100.0\n",
       "906           mmlu_chat.professional_law/correct_format                  100.0\n",
       "908    mmlu_chat.professional_accounting/correct_format                  100.0\n",
       "910                 mmlu_chat.prehistory/correct_format                  100.0\n",
       "912                 mmlu_chat.philosophy/correct_format                  100.0\n",
       "914                  mmlu_chat.nutrition/correct_format                  100.0\n",
       "916            mmlu_chat.moral_scenarios/correct_format                  100.0\n",
       "918             mmlu_chat.moral_disputes/correct_format                  100.0\n",
       "920              mmlu_chat.miscellaneous/correct_format                  100.0\n",
       "922           mmlu_chat.medical_genetics/correct_format                  100.0\n",
       "924                  mmlu_chat.marketing/correct_format                  100.0\n",
       "926                 mmlu_chat.management/correct_format                  100.0\n",
       "928           mmlu_chat.machine_learning/correct_format                  100.0\n",
       "930          mmlu_chat.logical_fallacies/correct_format                  100.0\n",
       "932              mmlu_chat.jurisprudence/correct_format                  100.0\n",
       "934          mmlu_chat.international_law/correct_format                  100.0\n",
       "936            mmlu_chat.human_sexuality/correct_format                  100.0\n",
       "938                mmlu_chat.human_aging/correct_format                  100.0\n",
       "940   mmlu_chat.high_school_world_history/correct_fo...                  100.0\n",
       "942     mmlu_chat.high_school_us_history/correct_format                  100.0\n",
       "944     mmlu_chat.high_school_statistics/correct_format                  100.0\n",
       "946     mmlu_chat.high_school_psychology/correct_format                  100.0\n",
       "948        mmlu_chat.high_school_physics/correct_format                  100.0\n",
       "950   mmlu_chat.high_school_microeconomics/correct_f...                  100.0\n",
       "952    mmlu_chat.high_school_mathematics/correct_format                  100.0\n",
       "954   mmlu_chat.high_school_macroeconomics/correct_f...                  100.0\n",
       "956   mmlu_chat.high_school_government_and_politics/...                  100.0\n",
       "958      mmlu_chat.high_school_geography/correct_format                  100.0\n",
       "960   mmlu_chat.high_school_european_history/correct...                  100.0\n",
       "962   mmlu_chat.high_school_computer_science/correct...                  100.0\n",
       "964      mmlu_chat.high_school_chemistry/correct_format                  100.0\n",
       "966        mmlu_chat.high_school_biology/correct_format                  100.0\n",
       "968               mmlu_chat.global_facts/correct_format                  100.0\n",
       "970               mmlu_chat.formal_logic/correct_format                  100.0\n",
       "972     mmlu_chat.elementary_mathematics/correct_format                  100.0\n",
       "974     mmlu_chat.electrical_engineering/correct_format                  100.0\n",
       "976               mmlu_chat.econometrics/correct_format                  100.0\n",
       "978         mmlu_chat.conceptual_physics/correct_format                  100.0\n",
       "980          mmlu_chat.computer_security/correct_format                  100.0\n",
       "982            mmlu_chat.college_physics/correct_format                  100.0\n",
       "984           mmlu_chat.college_medicine/correct_format                  100.0\n",
       "986        mmlu_chat.college_mathematics/correct_format                  100.0\n",
       "988   mmlu_chat.college_computer_science/correct_format                  100.0\n",
       "990          mmlu_chat.college_chemistry/correct_format                  100.0\n",
       "992            mmlu_chat.college_biology/correct_format                  100.0\n",
       "994         mmlu_chat.clinical_knowledge/correct_format                  100.0\n",
       "996            mmlu_chat.business_ethics/correct_format                  100.0\n",
       "998                  mmlu_chat.astronomy/correct_format                  100.0\n",
       "1000                   mmlu_chat.anatomy/correct_format                  100.0\n",
       "1002          mmlu_chat.abstract_algebra/correct_format                  100.0\n",
       "1004                           micro_avg/correct_format                  100.0\n",
       "1006                           macro_avg/correct_format                  100.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = metrics[metrics['benchmark_label'] == 'MMLU']\n",
    "phrase = 'correct_format'\n",
    "# metrics_df\n",
    "metrics_df['contains_phrase'] = metrics_df['metric_tag'].str.contains(phrase, case=False, na=False)\n",
    "metrics_df[metrics_df['contains_phrase'] == True][['metric_tag', 'metric_value_computed']]\n",
    "# metrics_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
