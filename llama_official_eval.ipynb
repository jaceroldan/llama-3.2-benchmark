{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 6 Eval\n",
    "Macro averaged MMLU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Subject  Is Correct\n",
      "0                      abstract_algebra    0.180000\n",
      "1                               anatomy    0.459259\n",
      "2                             astronomy    0.355263\n",
      "3                       business_ethics    0.490000\n",
      "4                    clinical_knowledge    0.381132\n",
      "5                       college_biology    0.416667\n",
      "6                     college_chemistry    0.330000\n",
      "7              college_computer_science    0.300000\n",
      "8                   college_mathematics    0.270000\n",
      "9                      college_medicine    0.404624\n",
      "10                      college_physics    0.225490\n",
      "11                    computer_security    0.550000\n",
      "12                   conceptual_physics    0.417021\n",
      "13                         econometrics    0.289474\n",
      "14               electrical_engineering    0.434483\n",
      "15               elementary_mathematics    0.309524\n",
      "16                         formal_logic    0.309524\n",
      "17                         global_facts    0.380000\n",
      "18                  high_school_biology    0.425806\n",
      "19                high_school_chemistry    0.369458\n",
      "20         high_school_computer_science    0.440000\n",
      "21         high_school_european_history    0.375758\n",
      "22                high_school_geography    0.444444\n",
      "23  high_school_government_and_politics    0.466321\n",
      "24           high_school_macroeconomics    0.325641\n",
      "25              high_school_mathematics    0.333333\n",
      "26           high_school_microeconomics    0.415966\n",
      "27                  high_school_physics    0.198675\n",
      "28               high_school_psychology    0.385321\n",
      "29               high_school_statistics    0.300926\n",
      "30               high_school_us_history    0.411765\n",
      "31            high_school_world_history    0.434599\n",
      "32                          human_aging    0.434978\n",
      "33                      human_sexuality    0.419847\n",
      "34                    international_law    0.404959\n",
      "35                        jurisprudence    0.462963\n",
      "36                    logical_fallacies    0.361963\n",
      "37                     machine_learning    0.383929\n",
      "38                           management    0.456311\n",
      "39                            marketing    0.589744\n",
      "40                     medical_genetics    0.490000\n",
      "41                        miscellaneous    0.544061\n",
      "42                       moral_disputes    0.323699\n",
      "43                      moral_scenarios    0.236872\n",
      "44                            nutrition    0.454248\n",
      "45                           philosophy    0.382637\n",
      "46                           prehistory    0.459877\n",
      "47              professional_accounting    0.255319\n",
      "48                     professional_law    0.224902\n",
      "49                professional_medicine    0.382353\n",
      "50              professional_psychology    0.377451\n",
      "51                     public_relations    0.481818\n",
      "52                     security_studies    0.424490\n",
      "53                            sociology    0.412935\n",
      "54                    us_foreign_policy    0.490000\n",
      "55                             virology    0.397590\n",
      "56                      world_religions    0.520468\n",
      "Macro Accuracy 0.3895419141766825\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run6.csv')\n",
    "class_accuracies = df.groupby('Subject')['Is Correct'].mean().reset_index()\n",
    "print(class_accuracies)\n",
    "\n",
    "macro_accuracy = class_accuracies['Is Correct'].mean()\n",
    "print('Macro Accuracy', macro_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 7 Eval\n",
    "Macro averaged MMLU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Subject  Is Correct\n",
      "0                      mmlu_chat.abstract_algebra    0.300000\n",
      "1                               mmlu_chat.anatomy    0.496296\n",
      "2                             mmlu_chat.astronomy    0.565789\n",
      "3                       mmlu_chat.business_ethics    0.460000\n",
      "4                    mmlu_chat.clinical_knowledge    0.471698\n",
      "5                       mmlu_chat.college_biology    0.500000\n",
      "6                     mmlu_chat.college_chemistry    0.350000\n",
      "7              mmlu_chat.college_computer_science    0.360000\n",
      "8                   mmlu_chat.college_mathematics    0.330000\n",
      "9                      mmlu_chat.college_medicine    0.421965\n",
      "10                      mmlu_chat.college_physics    0.196078\n",
      "11                    mmlu_chat.computer_security    0.570000\n",
      "12                   mmlu_chat.conceptual_physics    0.400000\n",
      "13                         mmlu_chat.econometrics    0.271930\n",
      "14               mmlu_chat.electrical_engineering    0.558621\n",
      "15               mmlu_chat.elementary_mathematics    0.251323\n",
      "16                         mmlu_chat.formal_logic    0.119048\n",
      "17                         mmlu_chat.global_facts    0.340000\n",
      "18                  mmlu_chat.high_school_biology    0.532258\n",
      "19                mmlu_chat.high_school_chemistry    0.403941\n",
      "20         mmlu_chat.high_school_computer_science    0.310000\n",
      "21         mmlu_chat.high_school_european_history    0.109091\n",
      "22                mmlu_chat.high_school_geography    0.611111\n",
      "23  mmlu_chat.high_school_government_and_politics    0.569948\n",
      "24           mmlu_chat.high_school_macroeconomics    0.374359\n",
      "25              mmlu_chat.high_school_mathematics    0.229630\n",
      "26           mmlu_chat.high_school_microeconomics    0.407563\n",
      "27                  mmlu_chat.high_school_physics    0.350993\n",
      "28               mmlu_chat.high_school_psychology    0.649541\n",
      "29               mmlu_chat.high_school_statistics    0.342593\n",
      "30               mmlu_chat.high_school_us_history    0.098039\n",
      "31            mmlu_chat.high_school_world_history    0.303797\n",
      "32                          mmlu_chat.human_aging    0.520179\n",
      "33                      mmlu_chat.human_sexuality    0.511450\n",
      "34                    mmlu_chat.international_law    0.636364\n",
      "35                        mmlu_chat.jurisprudence    0.546296\n",
      "36                    mmlu_chat.logical_fallacies    0.453988\n",
      "37                     mmlu_chat.machine_learning    0.339286\n",
      "38                           mmlu_chat.management    0.592233\n",
      "39                            mmlu_chat.marketing    0.692308\n",
      "40                     mmlu_chat.medical_genetics    0.570000\n",
      "41                        mmlu_chat.miscellaneous    0.618135\n",
      "42                       mmlu_chat.moral_disputes    0.500000\n",
      "43                      mmlu_chat.moral_scenarios    0.143017\n",
      "44                            mmlu_chat.nutrition    0.539216\n",
      "45                           mmlu_chat.philosophy    0.533762\n",
      "46                           mmlu_chat.prehistory    0.546296\n",
      "47              mmlu_chat.professional_accounting    0.347518\n",
      "48                     mmlu_chat.professional_law    0.342243\n",
      "49                mmlu_chat.professional_medicine    0.551471\n",
      "50              mmlu_chat.professional_psychology    0.421569\n",
      "51                     mmlu_chat.public_relations    0.463636\n",
      "52                     mmlu_chat.security_studies    0.436735\n",
      "53                            mmlu_chat.sociology    0.577114\n",
      "54                    mmlu_chat.us_foreign_policy    0.670000\n",
      "55                             mmlu_chat.virology    0.421687\n",
      "56                      mmlu_chat.world_religions    0.567251\n",
      "Macro Accuracy 0.4350415157264258\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('llama_mmlu_responses_run7.csv')\n",
    "class_accuracies = df.groupby('Subject')['Is Correct'].mean().reset_index()\n",
    "print(class_accuracies)\n",
    "\n",
    "macro_accuracy = class_accuracies['Is Correct'].mean()\n",
    "print('Macro Accuracy', macro_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
